{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_輕油案子.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/works/blob/master/Keras_%E8%BC%95%E6%B2%B9%E6%A1%88%E5%AD%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HnnkgwOKG4O",
        "colab_type": "code",
        "outputId": "485f37d8-b23e-4652-8eeb-237eeddb7667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn.metrics import r2_score\n",
        "from keras import backend\n",
        "from keras.layers import Dropout,Dense,Flatten\n",
        "from keras.models import Sequential\n",
        "import math\n",
        "import keras.backend as K\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVhGo3RiV7Bm",
        "colab_type": "text"
      },
      "source": [
        "# 讀取數據"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XaaAPz7KaEK",
        "colab_type": "code",
        "outputId": "1dca247d-bde3-4bb6-d0b2-2414198f4e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train = pd.read_csv('/content/gdrive/My Drive/台塑輕油案子/data/train_4565.csv',index_col=0)\n",
        "test = pd.read_csv('/content/gdrive/My Drive/台塑輕油案子/data/test_170.csv',index_col=0)\n",
        "train.append(test).head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T10</th>\n",
              "      <th>T50</th>\n",
              "      <th>T90</th>\n",
              "      <th>N+A</th>\n",
              "      <th>C5NP</th>\n",
              "      <th>C5IP</th>\n",
              "      <th>C5N</th>\n",
              "      <th>C6NP</th>\n",
              "      <th>C6IP</th>\n",
              "      <th>C6N</th>\n",
              "      <th>C6A</th>\n",
              "      <th>C7NP</th>\n",
              "      <th>C7IP</th>\n",
              "      <th>C7N</th>\n",
              "      <th>C7A</th>\n",
              "      <th>C8NP</th>\n",
              "      <th>C8IP</th>\n",
              "      <th>C8N</th>\n",
              "      <th>C8A</th>\n",
              "      <th>C9NP</th>\n",
              "      <th>C9IP</th>\n",
              "      <th>C9N</th>\n",
              "      <th>C9A</th>\n",
              "      <th>C10NP</th>\n",
              "      <th>C10IP</th>\n",
              "      <th>C10N</th>\n",
              "      <th>C10A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.5</td>\n",
              "      <td>119.2</td>\n",
              "      <td>146.5</td>\n",
              "      <td>31.978</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.059</td>\n",
              "      <td>5.293</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.819</td>\n",
              "      <td>0.494</td>\n",
              "      <td>10.395</td>\n",
              "      <td>8.070</td>\n",
              "      <td>6.411</td>\n",
              "      <td>2.917</td>\n",
              "      <td>9.138</td>\n",
              "      <td>9.649</td>\n",
              "      <td>4.810</td>\n",
              "      <td>5.373</td>\n",
              "      <td>6.405</td>\n",
              "      <td>9.759</td>\n",
              "      <td>4.590</td>\n",
              "      <td>3.661</td>\n",
              "      <td>0.875</td>\n",
              "      <td>5.257</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99.6</td>\n",
              "      <td>117.9</td>\n",
              "      <td>145.5</td>\n",
              "      <td>31.568</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.062</td>\n",
              "      <td>5.089</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.804</td>\n",
              "      <td>0.499</td>\n",
              "      <td>10.074</td>\n",
              "      <td>7.958</td>\n",
              "      <td>6.395</td>\n",
              "      <td>2.894</td>\n",
              "      <td>8.970</td>\n",
              "      <td>9.548</td>\n",
              "      <td>4.753</td>\n",
              "      <td>5.443</td>\n",
              "      <td>6.324</td>\n",
              "      <td>9.899</td>\n",
              "      <td>4.301</td>\n",
              "      <td>2.995</td>\n",
              "      <td>0.881</td>\n",
              "      <td>5.591</td>\n",
              "      <td>1.119</td>\n",
              "      <td>0.303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100.0</td>\n",
              "      <td>118.8</td>\n",
              "      <td>145.6</td>\n",
              "      <td>31.344</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.061</td>\n",
              "      <td>5.107</td>\n",
              "      <td>2.571</td>\n",
              "      <td>2.771</td>\n",
              "      <td>0.492</td>\n",
              "      <td>10.069</td>\n",
              "      <td>7.913</td>\n",
              "      <td>6.378</td>\n",
              "      <td>2.890</td>\n",
              "      <td>9.006</td>\n",
              "      <td>9.591</td>\n",
              "      <td>4.778</td>\n",
              "      <td>5.468</td>\n",
              "      <td>6.360</td>\n",
              "      <td>9.983</td>\n",
              "      <td>4.274</td>\n",
              "      <td>2.979</td>\n",
              "      <td>0.865</td>\n",
              "      <td>5.641</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100.4</td>\n",
              "      <td>118.6</td>\n",
              "      <td>142.9</td>\n",
              "      <td>31.453</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.060</td>\n",
              "      <td>4.894</td>\n",
              "      <td>2.497</td>\n",
              "      <td>2.650</td>\n",
              "      <td>0.469</td>\n",
              "      <td>10.015</td>\n",
              "      <td>7.685</td>\n",
              "      <td>6.376</td>\n",
              "      <td>2.866</td>\n",
              "      <td>9.133</td>\n",
              "      <td>9.708</td>\n",
              "      <td>4.889</td>\n",
              "      <td>5.510</td>\n",
              "      <td>6.444</td>\n",
              "      <td>10.182</td>\n",
              "      <td>4.420</td>\n",
              "      <td>2.964</td>\n",
              "      <td>0.830</td>\n",
              "      <td>5.637</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100.4</td>\n",
              "      <td>118.1</td>\n",
              "      <td>142.2</td>\n",
              "      <td>32.190</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.061</td>\n",
              "      <td>4.946</td>\n",
              "      <td>2.503</td>\n",
              "      <td>2.695</td>\n",
              "      <td>0.477</td>\n",
              "      <td>10.053</td>\n",
              "      <td>7.765</td>\n",
              "      <td>6.394</td>\n",
              "      <td>2.877</td>\n",
              "      <td>9.101</td>\n",
              "      <td>9.676</td>\n",
              "      <td>4.855</td>\n",
              "      <td>5.500</td>\n",
              "      <td>6.416</td>\n",
              "      <td>10.115</td>\n",
              "      <td>4.347</td>\n",
              "      <td>3.725</td>\n",
              "      <td>0.835</td>\n",
              "      <td>4.823</td>\n",
              "      <td>0.969</td>\n",
              "      <td>0.290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     T10    T50    T90     N+A   C5NP  ...    C9A  C10NP  C10IP   C10N   C10A\n",
              "2  100.5  119.2  146.5  31.978  0.272  ...  3.661  0.875  5.257  0.525  0.319\n",
              "3   99.6  117.9  145.5  31.568  0.297  ...  2.995  0.881  5.591  1.119  0.303\n",
              "4  100.0  118.8  145.6  31.344  0.262  ...  2.979  0.865  5.641  0.964  0.289\n",
              "5  100.4  118.6  142.9  31.453  0.224  ...  2.964  0.830  5.637  0.968  0.281\n",
              "6  100.4  118.1  142.2  32.190  0.243  ...  3.725  0.835  4.823  0.969  0.290\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsu1uqBBV9HP",
        "colab_type": "text"
      },
      "source": [
        "# 定義欄位"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t90tC8ZxVMNt",
        "colab_type": "code",
        "outputId": "3f4eaef0-57ce-406f-d340-d91ee37a345d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cols = train.columns.tolist()\n",
        "x_cols = cols[:4]\n",
        "y_cols = cols[4:]\n",
        "print(x_cols)\n",
        "print(y_cols)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['T10', 'T50', 'T90', 'N+A']\n",
            "['C5NP', 'C5IP', 'C5N', 'C6NP', 'C6IP', 'C6N', 'C6A', 'C7NP', 'C7IP', 'C7N', 'C7A', 'C8NP', 'C8IP', 'C8N', 'C8A', 'C9NP', 'C9IP', 'C9N', 'C9A', 'C10NP', 'C10IP', 'C10N', 'C10A']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wkAyPy1XvjH",
        "colab_type": "text"
      },
      "source": [
        "# 特徵縮放"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjR2AftBXvvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = StandardScaler()\n",
        "train[x_cols] = sc.fit_transform(train[x_cols])\n",
        "test[x_cols] = sc.transform(test[x_cols])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZC74GahLuG",
        "colab_type": "text"
      },
      "source": [
        "# y縮放"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Q35luvhL4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc_y = StandardScaler()\n",
        "train[y_cols] = sc_y.fit_transform(train[y_cols])\n",
        "test[y_cols] = sc_y.transform(test[y_cols])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jss7mRvfhgfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "14e6e536-0aaf-48a3-d5e0-119b0e549e5b"
      },
      "source": [
        "train.append(test).head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T10</th>\n",
              "      <th>T50</th>\n",
              "      <th>T90</th>\n",
              "      <th>N+A</th>\n",
              "      <th>C5NP</th>\n",
              "      <th>C5IP</th>\n",
              "      <th>C5N</th>\n",
              "      <th>C6NP</th>\n",
              "      <th>C6IP</th>\n",
              "      <th>C6N</th>\n",
              "      <th>C6A</th>\n",
              "      <th>C7NP</th>\n",
              "      <th>C7IP</th>\n",
              "      <th>C7N</th>\n",
              "      <th>C7A</th>\n",
              "      <th>C8NP</th>\n",
              "      <th>C8IP</th>\n",
              "      <th>C8N</th>\n",
              "      <th>C8A</th>\n",
              "      <th>C9NP</th>\n",
              "      <th>C9IP</th>\n",
              "      <th>C9N</th>\n",
              "      <th>C9A</th>\n",
              "      <th>C10NP</th>\n",
              "      <th>C10IP</th>\n",
              "      <th>C10N</th>\n",
              "      <th>C10A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.431894</td>\n",
              "      <td>0.537538</td>\n",
              "      <td>0.457373</td>\n",
              "      <td>-1.025830</td>\n",
              "      <td>-0.656696</td>\n",
              "      <td>-0.662401</td>\n",
              "      <td>-0.455957</td>\n",
              "      <td>1.388185</td>\n",
              "      <td>0.043335</td>\n",
              "      <td>-0.599718</td>\n",
              "      <td>-0.247984</td>\n",
              "      <td>0.945498</td>\n",
              "      <td>0.754116</td>\n",
              "      <td>-0.891516</td>\n",
              "      <td>-0.394642</td>\n",
              "      <td>0.753140</td>\n",
              "      <td>0.725334</td>\n",
              "      <td>-0.945362</td>\n",
              "      <td>-0.039664</td>\n",
              "      <td>0.994414</td>\n",
              "      <td>0.580943</td>\n",
              "      <td>-0.151680</td>\n",
              "      <td>0.765172</td>\n",
              "      <td>-0.054125</td>\n",
              "      <td>0.624958</td>\n",
              "      <td>-0.404661</td>\n",
              "      <td>-0.714551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.350890</td>\n",
              "      <td>0.336746</td>\n",
              "      <td>0.226896</td>\n",
              "      <td>-1.075101</td>\n",
              "      <td>-0.645038</td>\n",
              "      <td>-0.649892</td>\n",
              "      <td>-0.447299</td>\n",
              "      <td>1.220854</td>\n",
              "      <td>0.021172</td>\n",
              "      <td>-0.607274</td>\n",
              "      <td>-0.245389</td>\n",
              "      <td>0.807423</td>\n",
              "      <td>0.680581</td>\n",
              "      <td>-0.895034</td>\n",
              "      <td>-0.413999</td>\n",
              "      <td>0.667751</td>\n",
              "      <td>0.676452</td>\n",
              "      <td>-0.970661</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.945172</td>\n",
              "      <td>0.652603</td>\n",
              "      <td>-0.446794</td>\n",
              "      <td>-0.005734</td>\n",
              "      <td>-0.041679</td>\n",
              "      <td>0.904367</td>\n",
              "      <td>1.252672</td>\n",
              "      <td>-0.746137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.386892</td>\n",
              "      <td>0.475756</td>\n",
              "      <td>0.249944</td>\n",
              "      <td>-1.102020</td>\n",
              "      <td>-0.661359</td>\n",
              "      <td>-0.666351</td>\n",
              "      <td>-0.450185</td>\n",
              "      <td>1.235618</td>\n",
              "      <td>0.043903</td>\n",
              "      <td>-0.623898</td>\n",
              "      <td>-0.249022</td>\n",
              "      <td>0.805272</td>\n",
              "      <td>0.651035</td>\n",
              "      <td>-0.898772</td>\n",
              "      <td>-0.417366</td>\n",
              "      <td>0.686048</td>\n",
              "      <td>0.697263</td>\n",
              "      <td>-0.959565</td>\n",
              "      <td>0.020970</td>\n",
              "      <td>0.967057</td>\n",
              "      <td>0.695599</td>\n",
              "      <td>-0.474365</td>\n",
              "      <td>-0.024254</td>\n",
              "      <td>-0.074868</td>\n",
              "      <td>0.946195</td>\n",
              "      <td>0.820203</td>\n",
              "      <td>-0.773775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.422893</td>\n",
              "      <td>0.444864</td>\n",
              "      <td>-0.372346</td>\n",
              "      <td>-1.088921</td>\n",
              "      <td>-0.679080</td>\n",
              "      <td>-0.680176</td>\n",
              "      <td>-0.453071</td>\n",
              "      <td>1.060905</td>\n",
              "      <td>0.001851</td>\n",
              "      <td>-0.684850</td>\n",
              "      <td>-0.260961</td>\n",
              "      <td>0.782045</td>\n",
              "      <td>0.501339</td>\n",
              "      <td>-0.899212</td>\n",
              "      <td>-0.437564</td>\n",
              "      <td>0.750598</td>\n",
              "      <td>0.753889</td>\n",
              "      <td>-0.910299</td>\n",
              "      <td>0.047776</td>\n",
              "      <td>1.018123</td>\n",
              "      <td>0.797459</td>\n",
              "      <td>-0.325276</td>\n",
              "      <td>-0.041617</td>\n",
              "      <td>-0.147468</td>\n",
              "      <td>0.942849</td>\n",
              "      <td>0.831364</td>\n",
              "      <td>-0.789568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.422893</td>\n",
              "      <td>0.367637</td>\n",
              "      <td>-0.533680</td>\n",
              "      <td>-1.000353</td>\n",
              "      <td>-0.670220</td>\n",
              "      <td>-0.672276</td>\n",
              "      <td>-0.450185</td>\n",
              "      <td>1.103558</td>\n",
              "      <td>0.005260</td>\n",
              "      <td>-0.662182</td>\n",
              "      <td>-0.256809</td>\n",
              "      <td>0.798390</td>\n",
              "      <td>0.553864</td>\n",
              "      <td>-0.895254</td>\n",
              "      <td>-0.428307</td>\n",
              "      <td>0.734334</td>\n",
              "      <td>0.738401</td>\n",
              "      <td>-0.925390</td>\n",
              "      <td>0.041394</td>\n",
              "      <td>1.001101</td>\n",
              "      <td>0.763164</td>\n",
              "      <td>-0.399821</td>\n",
              "      <td>0.839253</td>\n",
              "      <td>-0.137097</td>\n",
              "      <td>0.261892</td>\n",
              "      <td>0.834154</td>\n",
              "      <td>-0.771800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        T10       T50       T90  ...     C10IP      C10N      C10A\n",
              "2  0.431894  0.537538  0.457373  ...  0.624958 -0.404661 -0.714551\n",
              "3  0.350890  0.336746  0.226896  ...  0.904367  1.252672 -0.746137\n",
              "4  0.386892  0.475756  0.249944  ...  0.946195  0.820203 -0.773775\n",
              "5  0.422893  0.444864 -0.372346  ...  0.942849  0.831364 -0.789568\n",
              "6  0.422893  0.367637 -0.533680  ...  0.261892  0.834154 -0.771800\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFb9YwXsaQV6",
        "colab_type": "text"
      },
      "source": [
        "# 拆分訓練及驗證集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCYgndPaQ_m",
        "colab_type": "code",
        "outputId": "f84ac72d-8192-46c4-f010-ba9203f6baa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train ,val= train_test_split(train,test_size=0.3,random_state=42)\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3195, 27)\n",
            "(1370, 27)\n",
            "(170, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOgxt9-FV4Cb",
        "colab_type": "text"
      },
      "source": [
        "# 建立模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAbsTZsicFvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    error = y_true-y_pred\n",
        "    sum_error = K.sum(y_pred)-K.sum(y_true)\n",
        "    mse = K.mean(K.square(error))\n",
        "    mae = K.mean(K.abs(error))\n",
        "    sum_constraint = K.abs(sum_error)\n",
        "    return mae\n",
        "def custom_rmse(y_true,y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_true-y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBzXgE3SRgLB",
        "colab_type": "code",
        "outputId": "8acdff08-afe9-4b0f-f036-b1d4f65dae39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128,activation='relu',input_dim=4))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(23,activation=None))\n",
        "model.compile(optimizer='Adam',loss=custom_loss,metrics=[custom_rmse])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               640       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 23)                2967      \n",
            "=================================================================\n",
            "Total params: 36,631\n",
            "Trainable params: 36,631\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IxOKHnjVmCe",
        "colab_type": "text"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMRXS3Qopipr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_loss', \n",
        "                   min_delta=0, \n",
        "                   patience=30, \n",
        "                   mode='min' , \n",
        "                   restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR5bagHgNOUE",
        "colab_type": "code",
        "outputId": "de390733-0d3d-455b-9e9e-af9668fa62e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(train[x_cols],train[y_cols],\n",
        "                    epochs=1000,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(val[x_cols],val[y_cols]),\n",
        "                    callbacks=[es])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 3195 samples, validate on 1370 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "3195/3195 [==============================] - 1s 335us/step - loss: 0.7001 - custom_rmse: 0.9882 - val_loss: 0.6482 - val_custom_rmse: 0.9003\n",
            "Epoch 2/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.6121 - custom_rmse: 0.9024 - val_loss: 0.5314 - val_custom_rmse: 0.7922\n",
            "Epoch 3/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.4921 - custom_rmse: 0.8020 - val_loss: 0.4232 - val_custom_rmse: 0.7064\n",
            "Epoch 4/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.3995 - custom_rmse: 0.7341 - val_loss: 0.3504 - val_custom_rmse: 0.6460\n",
            "Epoch 5/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.3431 - custom_rmse: 0.6899 - val_loss: 0.3173 - val_custom_rmse: 0.6186\n",
            "Epoch 6/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.3174 - custom_rmse: 0.6669 - val_loss: 0.2966 - val_custom_rmse: 0.6006\n",
            "Epoch 7/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.3004 - custom_rmse: 0.6535 - val_loss: 0.2864 - val_custom_rmse: 0.5888\n",
            "Epoch 8/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2904 - custom_rmse: 0.6392 - val_loss: 0.2791 - val_custom_rmse: 0.5848\n",
            "Epoch 9/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2832 - custom_rmse: 0.6336 - val_loss: 0.2734 - val_custom_rmse: 0.5787\n",
            "Epoch 10/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2779 - custom_rmse: 0.6283 - val_loss: 0.2692 - val_custom_rmse: 0.5746\n",
            "Epoch 11/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2733 - custom_rmse: 0.6261 - val_loss: 0.2653 - val_custom_rmse: 0.5701\n",
            "Epoch 12/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2690 - custom_rmse: 0.6204 - val_loss: 0.2619 - val_custom_rmse: 0.5673\n",
            "Epoch 13/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2655 - custom_rmse: 0.6175 - val_loss: 0.2592 - val_custom_rmse: 0.5660\n",
            "Epoch 14/1000\n",
            "3195/3195 [==============================] - 0s 16us/step - loss: 0.2627 - custom_rmse: 0.6205 - val_loss: 0.2565 - val_custom_rmse: 0.5625\n",
            "Epoch 15/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2604 - custom_rmse: 0.6116 - val_loss: 0.2545 - val_custom_rmse: 0.5615\n",
            "Epoch 16/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2583 - custom_rmse: 0.6146 - val_loss: 0.2528 - val_custom_rmse: 0.5601\n",
            "Epoch 17/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2562 - custom_rmse: 0.6112 - val_loss: 0.2511 - val_custom_rmse: 0.5579\n",
            "Epoch 18/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2540 - custom_rmse: 0.6112 - val_loss: 0.2490 - val_custom_rmse: 0.5571\n",
            "Epoch 19/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2525 - custom_rmse: 0.6113 - val_loss: 0.2476 - val_custom_rmse: 0.5562\n",
            "Epoch 20/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2508 - custom_rmse: 0.6113 - val_loss: 0.2469 - val_custom_rmse: 0.5548\n",
            "Epoch 21/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2498 - custom_rmse: 0.6048 - val_loss: 0.2457 - val_custom_rmse: 0.5537\n",
            "Epoch 22/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2486 - custom_rmse: 0.6003 - val_loss: 0.2447 - val_custom_rmse: 0.5528\n",
            "Epoch 23/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2470 - custom_rmse: 0.6038 - val_loss: 0.2440 - val_custom_rmse: 0.5512\n",
            "Epoch 24/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2458 - custom_rmse: 0.6020 - val_loss: 0.2426 - val_custom_rmse: 0.5497\n",
            "Epoch 25/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2447 - custom_rmse: 0.6034 - val_loss: 0.2413 - val_custom_rmse: 0.5484\n",
            "Epoch 26/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2433 - custom_rmse: 0.6004 - val_loss: 0.2406 - val_custom_rmse: 0.5466\n",
            "Epoch 27/1000\n",
            "3195/3195 [==============================] - 0s 16us/step - loss: 0.2424 - custom_rmse: 0.5972 - val_loss: 0.2396 - val_custom_rmse: 0.5457\n",
            "Epoch 28/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2415 - custom_rmse: 0.5933 - val_loss: 0.2391 - val_custom_rmse: 0.5435\n",
            "Epoch 29/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2405 - custom_rmse: 0.5960 - val_loss: 0.2383 - val_custom_rmse: 0.5434\n",
            "Epoch 30/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2398 - custom_rmse: 0.5948 - val_loss: 0.2380 - val_custom_rmse: 0.5430\n",
            "Epoch 31/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2387 - custom_rmse: 0.5940 - val_loss: 0.2372 - val_custom_rmse: 0.5420\n",
            "Epoch 32/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2382 - custom_rmse: 0.5908 - val_loss: 0.2363 - val_custom_rmse: 0.5411\n",
            "Epoch 33/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2370 - custom_rmse: 0.5893 - val_loss: 0.2360 - val_custom_rmse: 0.5404\n",
            "Epoch 34/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2368 - custom_rmse: 0.5924 - val_loss: 0.2349 - val_custom_rmse: 0.5390\n",
            "Epoch 35/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2360 - custom_rmse: 0.5836 - val_loss: 0.2351 - val_custom_rmse: 0.5388\n",
            "Epoch 36/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2353 - custom_rmse: 0.5895 - val_loss: 0.2345 - val_custom_rmse: 0.5378\n",
            "Epoch 37/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2348 - custom_rmse: 0.5896 - val_loss: 0.2343 - val_custom_rmse: 0.5361\n",
            "Epoch 38/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2342 - custom_rmse: 0.5877 - val_loss: 0.2329 - val_custom_rmse: 0.5355\n",
            "Epoch 39/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2335 - custom_rmse: 0.5842 - val_loss: 0.2325 - val_custom_rmse: 0.5344\n",
            "Epoch 40/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2331 - custom_rmse: 0.5865 - val_loss: 0.2324 - val_custom_rmse: 0.5344\n",
            "Epoch 41/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2324 - custom_rmse: 0.5857 - val_loss: 0.2320 - val_custom_rmse: 0.5332\n",
            "Epoch 42/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2321 - custom_rmse: 0.5789 - val_loss: 0.2326 - val_custom_rmse: 0.5328\n",
            "Epoch 43/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2318 - custom_rmse: 0.5795 - val_loss: 0.2316 - val_custom_rmse: 0.5309\n",
            "Epoch 44/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2312 - custom_rmse: 0.5728 - val_loss: 0.2318 - val_custom_rmse: 0.5310\n",
            "Epoch 45/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2309 - custom_rmse: 0.5828 - val_loss: 0.2313 - val_custom_rmse: 0.5298\n",
            "Epoch 46/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2304 - custom_rmse: 0.5812 - val_loss: 0.2311 - val_custom_rmse: 0.5295\n",
            "Epoch 47/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2305 - custom_rmse: 0.5797 - val_loss: 0.2309 - val_custom_rmse: 0.5286\n",
            "Epoch 48/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2300 - custom_rmse: 0.5784 - val_loss: 0.2304 - val_custom_rmse: 0.5277\n",
            "Epoch 49/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2294 - custom_rmse: 0.5795 - val_loss: 0.2298 - val_custom_rmse: 0.5276\n",
            "Epoch 50/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2288 - custom_rmse: 0.5766 - val_loss: 0.2301 - val_custom_rmse: 0.5260\n",
            "Epoch 51/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2289 - custom_rmse: 0.5779 - val_loss: 0.2296 - val_custom_rmse: 0.5250\n",
            "Epoch 52/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2289 - custom_rmse: 0.5721 - val_loss: 0.2287 - val_custom_rmse: 0.5246\n",
            "Epoch 53/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2283 - custom_rmse: 0.5742 - val_loss: 0.2288 - val_custom_rmse: 0.5249\n",
            "Epoch 54/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2274 - custom_rmse: 0.5719 - val_loss: 0.2285 - val_custom_rmse: 0.5222\n",
            "Epoch 55/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2272 - custom_rmse: 0.5737 - val_loss: 0.2292 - val_custom_rmse: 0.5230\n",
            "Epoch 56/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2273 - custom_rmse: 0.5745 - val_loss: 0.2279 - val_custom_rmse: 0.5221\n",
            "Epoch 57/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2267 - custom_rmse: 0.5688 - val_loss: 0.2281 - val_custom_rmse: 0.5222\n",
            "Epoch 58/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2263 - custom_rmse: 0.5715 - val_loss: 0.2275 - val_custom_rmse: 0.5205\n",
            "Epoch 59/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2259 - custom_rmse: 0.5654 - val_loss: 0.2284 - val_custom_rmse: 0.5214\n",
            "Epoch 60/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2261 - custom_rmse: 0.5677 - val_loss: 0.2270 - val_custom_rmse: 0.5206\n",
            "Epoch 61/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2251 - custom_rmse: 0.5687 - val_loss: 0.2276 - val_custom_rmse: 0.5201\n",
            "Epoch 62/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2257 - custom_rmse: 0.5691 - val_loss: 0.2271 - val_custom_rmse: 0.5189\n",
            "Epoch 63/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2250 - custom_rmse: 0.5705 - val_loss: 0.2262 - val_custom_rmse: 0.5186\n",
            "Epoch 64/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2245 - custom_rmse: 0.5645 - val_loss: 0.2260 - val_custom_rmse: 0.5181\n",
            "Epoch 65/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2243 - custom_rmse: 0.5669 - val_loss: 0.2258 - val_custom_rmse: 0.5170\n",
            "Epoch 66/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2238 - custom_rmse: 0.5651 - val_loss: 0.2257 - val_custom_rmse: 0.5172\n",
            "Epoch 67/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2231 - custom_rmse: 0.5635 - val_loss: 0.2252 - val_custom_rmse: 0.5165\n",
            "Epoch 68/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2226 - custom_rmse: 0.5639 - val_loss: 0.2255 - val_custom_rmse: 0.5165\n",
            "Epoch 69/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2227 - custom_rmse: 0.5598 - val_loss: 0.2252 - val_custom_rmse: 0.5148\n",
            "Epoch 70/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2226 - custom_rmse: 0.5624 - val_loss: 0.2247 - val_custom_rmse: 0.5149\n",
            "Epoch 71/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2219 - custom_rmse: 0.5621 - val_loss: 0.2248 - val_custom_rmse: 0.5142\n",
            "Epoch 72/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2220 - custom_rmse: 0.5600 - val_loss: 0.2254 - val_custom_rmse: 0.5141\n",
            "Epoch 73/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2219 - custom_rmse: 0.5631 - val_loss: 0.2251 - val_custom_rmse: 0.5125\n",
            "Epoch 74/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.2218 - custom_rmse: 0.5600 - val_loss: 0.2244 - val_custom_rmse: 0.5132\n",
            "Epoch 75/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2216 - custom_rmse: 0.5589 - val_loss: 0.2242 - val_custom_rmse: 0.5120\n",
            "Epoch 76/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2214 - custom_rmse: 0.5638 - val_loss: 0.2244 - val_custom_rmse: 0.5120\n",
            "Epoch 77/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2207 - custom_rmse: 0.5532 - val_loss: 0.2237 - val_custom_rmse: 0.5111\n",
            "Epoch 78/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2201 - custom_rmse: 0.5632 - val_loss: 0.2235 - val_custom_rmse: 0.5098\n",
            "Epoch 79/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2201 - custom_rmse: 0.5522 - val_loss: 0.2228 - val_custom_rmse: 0.5092\n",
            "Epoch 80/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2198 - custom_rmse: 0.5570 - val_loss: 0.2230 - val_custom_rmse: 0.5083\n",
            "Epoch 81/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2195 - custom_rmse: 0.5550 - val_loss: 0.2230 - val_custom_rmse: 0.5086\n",
            "Epoch 82/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2192 - custom_rmse: 0.5571 - val_loss: 0.2221 - val_custom_rmse: 0.5076\n",
            "Epoch 83/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2188 - custom_rmse: 0.5567 - val_loss: 0.2232 - val_custom_rmse: 0.5076\n",
            "Epoch 84/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2188 - custom_rmse: 0.5497 - val_loss: 0.2224 - val_custom_rmse: 0.5068\n",
            "Epoch 85/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2189 - custom_rmse: 0.5578 - val_loss: 0.2223 - val_custom_rmse: 0.5061\n",
            "Epoch 86/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2187 - custom_rmse: 0.5573 - val_loss: 0.2219 - val_custom_rmse: 0.5056\n",
            "Epoch 87/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2181 - custom_rmse: 0.5466 - val_loss: 0.2218 - val_custom_rmse: 0.5062\n",
            "Epoch 88/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2183 - custom_rmse: 0.5519 - val_loss: 0.2211 - val_custom_rmse: 0.5051\n",
            "Epoch 89/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2174 - custom_rmse: 0.5514 - val_loss: 0.2211 - val_custom_rmse: 0.5050\n",
            "Epoch 90/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2170 - custom_rmse: 0.5473 - val_loss: 0.2209 - val_custom_rmse: 0.5045\n",
            "Epoch 91/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2167 - custom_rmse: 0.5507 - val_loss: 0.2208 - val_custom_rmse: 0.5049\n",
            "Epoch 92/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2165 - custom_rmse: 0.5538 - val_loss: 0.2203 - val_custom_rmse: 0.5041\n",
            "Epoch 93/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2162 - custom_rmse: 0.5524 - val_loss: 0.2206 - val_custom_rmse: 0.5048\n",
            "Epoch 94/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2163 - custom_rmse: 0.5559 - val_loss: 0.2206 - val_custom_rmse: 0.5041\n",
            "Epoch 95/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2168 - custom_rmse: 0.5548 - val_loss: 0.2204 - val_custom_rmse: 0.5037\n",
            "Epoch 96/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2165 - custom_rmse: 0.5547 - val_loss: 0.2199 - val_custom_rmse: 0.5034\n",
            "Epoch 97/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2154 - custom_rmse: 0.5487 - val_loss: 0.2198 - val_custom_rmse: 0.5036\n",
            "Epoch 98/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2152 - custom_rmse: 0.5476 - val_loss: 0.2198 - val_custom_rmse: 0.5031\n",
            "Epoch 99/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2153 - custom_rmse: 0.5487 - val_loss: 0.2199 - val_custom_rmse: 0.5031\n",
            "Epoch 100/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2151 - custom_rmse: 0.5538 - val_loss: 0.2202 - val_custom_rmse: 0.5033\n",
            "Epoch 101/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2152 - custom_rmse: 0.5489 - val_loss: 0.2194 - val_custom_rmse: 0.5019\n",
            "Epoch 102/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2153 - custom_rmse: 0.5525 - val_loss: 0.2195 - val_custom_rmse: 0.5028\n",
            "Epoch 103/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2146 - custom_rmse: 0.5503 - val_loss: 0.2192 - val_custom_rmse: 0.5022\n",
            "Epoch 104/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2143 - custom_rmse: 0.5520 - val_loss: 0.2196 - val_custom_rmse: 0.5019\n",
            "Epoch 105/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2144 - custom_rmse: 0.5450 - val_loss: 0.2193 - val_custom_rmse: 0.5017\n",
            "Epoch 106/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2144 - custom_rmse: 0.5499 - val_loss: 0.2204 - val_custom_rmse: 0.5024\n",
            "Epoch 107/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2146 - custom_rmse: 0.5507 - val_loss: 0.2189 - val_custom_rmse: 0.5008\n",
            "Epoch 108/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2141 - custom_rmse: 0.5485 - val_loss: 0.2191 - val_custom_rmse: 0.5011\n",
            "Epoch 109/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2141 - custom_rmse: 0.5366 - val_loss: 0.2192 - val_custom_rmse: 0.5012\n",
            "Epoch 110/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2135 - custom_rmse: 0.5469 - val_loss: 0.2184 - val_custom_rmse: 0.4999\n",
            "Epoch 111/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2130 - custom_rmse: 0.5490 - val_loss: 0.2186 - val_custom_rmse: 0.5004\n",
            "Epoch 112/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2134 - custom_rmse: 0.5491 - val_loss: 0.2182 - val_custom_rmse: 0.4999\n",
            "Epoch 113/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2130 - custom_rmse: 0.5480 - val_loss: 0.2189 - val_custom_rmse: 0.5002\n",
            "Epoch 114/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2135 - custom_rmse: 0.5461 - val_loss: 0.2197 - val_custom_rmse: 0.5011\n",
            "Epoch 115/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2133 - custom_rmse: 0.5490 - val_loss: 0.2187 - val_custom_rmse: 0.5001\n",
            "Epoch 116/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2132 - custom_rmse: 0.5486 - val_loss: 0.2192 - val_custom_rmse: 0.5005\n",
            "Epoch 117/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2133 - custom_rmse: 0.5453 - val_loss: 0.2194 - val_custom_rmse: 0.5007\n",
            "Epoch 118/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2127 - custom_rmse: 0.5419 - val_loss: 0.2192 - val_custom_rmse: 0.4998\n",
            "Epoch 119/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2126 - custom_rmse: 0.5453 - val_loss: 0.2187 - val_custom_rmse: 0.4993\n",
            "Epoch 120/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2124 - custom_rmse: 0.5401 - val_loss: 0.2182 - val_custom_rmse: 0.4987\n",
            "Epoch 121/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2119 - custom_rmse: 0.5462 - val_loss: 0.2187 - val_custom_rmse: 0.4993\n",
            "Epoch 122/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.2122 - custom_rmse: 0.5469 - val_loss: 0.2177 - val_custom_rmse: 0.4983\n",
            "Epoch 123/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2117 - custom_rmse: 0.5449 - val_loss: 0.2174 - val_custom_rmse: 0.4985\n",
            "Epoch 124/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2115 - custom_rmse: 0.5491 - val_loss: 0.2176 - val_custom_rmse: 0.4982\n",
            "Epoch 125/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2118 - custom_rmse: 0.5477 - val_loss: 0.2178 - val_custom_rmse: 0.4984\n",
            "Epoch 126/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2116 - custom_rmse: 0.5431 - val_loss: 0.2180 - val_custom_rmse: 0.4981\n",
            "Epoch 127/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2118 - custom_rmse: 0.5457 - val_loss: 0.2177 - val_custom_rmse: 0.4981\n",
            "Epoch 128/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2108 - custom_rmse: 0.5457 - val_loss: 0.2166 - val_custom_rmse: 0.4969\n",
            "Epoch 129/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2107 - custom_rmse: 0.5434 - val_loss: 0.2172 - val_custom_rmse: 0.4973\n",
            "Epoch 130/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2105 - custom_rmse: 0.5430 - val_loss: 0.2171 - val_custom_rmse: 0.4972\n",
            "Epoch 131/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2106 - custom_rmse: 0.5402 - val_loss: 0.2167 - val_custom_rmse: 0.4967\n",
            "Epoch 132/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2107 - custom_rmse: 0.5462 - val_loss: 0.2173 - val_custom_rmse: 0.4967\n",
            "Epoch 133/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2104 - custom_rmse: 0.5464 - val_loss: 0.2174 - val_custom_rmse: 0.4974\n",
            "Epoch 134/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2106 - custom_rmse: 0.5428 - val_loss: 0.2171 - val_custom_rmse: 0.4963\n",
            "Epoch 135/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2101 - custom_rmse: 0.5460 - val_loss: 0.2174 - val_custom_rmse: 0.4974\n",
            "Epoch 136/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2104 - custom_rmse: 0.5415 - val_loss: 0.2177 - val_custom_rmse: 0.4968\n",
            "Epoch 137/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2102 - custom_rmse: 0.5450 - val_loss: 0.2169 - val_custom_rmse: 0.4961\n",
            "Epoch 138/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2105 - custom_rmse: 0.5358 - val_loss: 0.2173 - val_custom_rmse: 0.4964\n",
            "Epoch 139/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2104 - custom_rmse: 0.5446 - val_loss: 0.2172 - val_custom_rmse: 0.4952\n",
            "Epoch 140/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2101 - custom_rmse: 0.5367 - val_loss: 0.2166 - val_custom_rmse: 0.4948\n",
            "Epoch 141/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2095 - custom_rmse: 0.5360 - val_loss: 0.2167 - val_custom_rmse: 0.4942\n",
            "Epoch 142/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2093 - custom_rmse: 0.5377 - val_loss: 0.2158 - val_custom_rmse: 0.4940\n",
            "Epoch 143/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2091 - custom_rmse: 0.5364 - val_loss: 0.2168 - val_custom_rmse: 0.4954\n",
            "Epoch 144/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2095 - custom_rmse: 0.5417 - val_loss: 0.2170 - val_custom_rmse: 0.4955\n",
            "Epoch 145/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2099 - custom_rmse: 0.5435 - val_loss: 0.2165 - val_custom_rmse: 0.4953\n",
            "Epoch 146/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2088 - custom_rmse: 0.5427 - val_loss: 0.2156 - val_custom_rmse: 0.4943\n",
            "Epoch 147/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2083 - custom_rmse: 0.5357 - val_loss: 0.2156 - val_custom_rmse: 0.4939\n",
            "Epoch 148/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2085 - custom_rmse: 0.5435 - val_loss: 0.2161 - val_custom_rmse: 0.4942\n",
            "Epoch 149/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2089 - custom_rmse: 0.5358 - val_loss: 0.2167 - val_custom_rmse: 0.4944\n",
            "Epoch 150/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2089 - custom_rmse: 0.5422 - val_loss: 0.2156 - val_custom_rmse: 0.4933\n",
            "Epoch 151/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2086 - custom_rmse: 0.5405 - val_loss: 0.2156 - val_custom_rmse: 0.4933\n",
            "Epoch 152/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2084 - custom_rmse: 0.5334 - val_loss: 0.2152 - val_custom_rmse: 0.4935\n",
            "Epoch 153/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2078 - custom_rmse: 0.5397 - val_loss: 0.2153 - val_custom_rmse: 0.4931\n",
            "Epoch 154/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2076 - custom_rmse: 0.5336 - val_loss: 0.2154 - val_custom_rmse: 0.4930\n",
            "Epoch 155/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2075 - custom_rmse: 0.5369 - val_loss: 0.2155 - val_custom_rmse: 0.4928\n",
            "Epoch 156/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2076 - custom_rmse: 0.5395 - val_loss: 0.2148 - val_custom_rmse: 0.4927\n",
            "Epoch 157/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2069 - custom_rmse: 0.5378 - val_loss: 0.2152 - val_custom_rmse: 0.4933\n",
            "Epoch 158/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2075 - custom_rmse: 0.5383 - val_loss: 0.2148 - val_custom_rmse: 0.4926\n",
            "Epoch 159/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2069 - custom_rmse: 0.5323 - val_loss: 0.2150 - val_custom_rmse: 0.4928\n",
            "Epoch 160/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2070 - custom_rmse: 0.5393 - val_loss: 0.2154 - val_custom_rmse: 0.4931\n",
            "Epoch 161/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2072 - custom_rmse: 0.5364 - val_loss: 0.2146 - val_custom_rmse: 0.4925\n",
            "Epoch 162/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2068 - custom_rmse: 0.5383 - val_loss: 0.2147 - val_custom_rmse: 0.4926\n",
            "Epoch 163/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2067 - custom_rmse: 0.5404 - val_loss: 0.2147 - val_custom_rmse: 0.4920\n",
            "Epoch 164/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2065 - custom_rmse: 0.5285 - val_loss: 0.2157 - val_custom_rmse: 0.4934\n",
            "Epoch 165/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2074 - custom_rmse: 0.5373 - val_loss: 0.2148 - val_custom_rmse: 0.4917\n",
            "Epoch 166/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2070 - custom_rmse: 0.5404 - val_loss: 0.2145 - val_custom_rmse: 0.4915\n",
            "Epoch 167/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2067 - custom_rmse: 0.5384 - val_loss: 0.2150 - val_custom_rmse: 0.4923\n",
            "Epoch 168/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2069 - custom_rmse: 0.5389 - val_loss: 0.2151 - val_custom_rmse: 0.4917\n",
            "Epoch 169/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2069 - custom_rmse: 0.5393 - val_loss: 0.2146 - val_custom_rmse: 0.4915\n",
            "Epoch 170/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2060 - custom_rmse: 0.5369 - val_loss: 0.2144 - val_custom_rmse: 0.4922\n",
            "Epoch 171/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2056 - custom_rmse: 0.5394 - val_loss: 0.2152 - val_custom_rmse: 0.4913\n",
            "Epoch 172/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2060 - custom_rmse: 0.5366 - val_loss: 0.2146 - val_custom_rmse: 0.4913\n",
            "Epoch 173/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2060 - custom_rmse: 0.5317 - val_loss: 0.2149 - val_custom_rmse: 0.4910\n",
            "Epoch 174/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2062 - custom_rmse: 0.5369 - val_loss: 0.2145 - val_custom_rmse: 0.4907\n",
            "Epoch 175/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2059 - custom_rmse: 0.5374 - val_loss: 0.2139 - val_custom_rmse: 0.4905\n",
            "Epoch 176/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2057 - custom_rmse: 0.5345 - val_loss: 0.2139 - val_custom_rmse: 0.4905\n",
            "Epoch 177/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2056 - custom_rmse: 0.5399 - val_loss: 0.2150 - val_custom_rmse: 0.4902\n",
            "Epoch 178/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2062 - custom_rmse: 0.5343 - val_loss: 0.2155 - val_custom_rmse: 0.4909\n",
            "Epoch 179/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2066 - custom_rmse: 0.5322 - val_loss: 0.2146 - val_custom_rmse: 0.4891\n",
            "Epoch 180/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2060 - custom_rmse: 0.5381 - val_loss: 0.2146 - val_custom_rmse: 0.4899\n",
            "Epoch 181/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2063 - custom_rmse: 0.5331 - val_loss: 0.2155 - val_custom_rmse: 0.4912\n",
            "Epoch 182/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2056 - custom_rmse: 0.5371 - val_loss: 0.2134 - val_custom_rmse: 0.4890\n",
            "Epoch 183/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2050 - custom_rmse: 0.5351 - val_loss: 0.2148 - val_custom_rmse: 0.4892\n",
            "Epoch 184/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2055 - custom_rmse: 0.5354 - val_loss: 0.2135 - val_custom_rmse: 0.4900\n",
            "Epoch 185/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2048 - custom_rmse: 0.5377 - val_loss: 0.2133 - val_custom_rmse: 0.4893\n",
            "Epoch 186/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2046 - custom_rmse: 0.5358 - val_loss: 0.2137 - val_custom_rmse: 0.4898\n",
            "Epoch 187/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2044 - custom_rmse: 0.5337 - val_loss: 0.2135 - val_custom_rmse: 0.4886\n",
            "Epoch 188/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2049 - custom_rmse: 0.5386 - val_loss: 0.2137 - val_custom_rmse: 0.4883\n",
            "Epoch 189/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2052 - custom_rmse: 0.5305 - val_loss: 0.2135 - val_custom_rmse: 0.4886\n",
            "Epoch 190/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2049 - custom_rmse: 0.5346 - val_loss: 0.2132 - val_custom_rmse: 0.4891\n",
            "Epoch 191/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2044 - custom_rmse: 0.5272 - val_loss: 0.2133 - val_custom_rmse: 0.4883\n",
            "Epoch 192/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2044 - custom_rmse: 0.5256 - val_loss: 0.2132 - val_custom_rmse: 0.4888\n",
            "Epoch 193/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2043 - custom_rmse: 0.5327 - val_loss: 0.2128 - val_custom_rmse: 0.4882\n",
            "Epoch 194/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.2040 - custom_rmse: 0.5331 - val_loss: 0.2134 - val_custom_rmse: 0.4891\n",
            "Epoch 195/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2041 - custom_rmse: 0.5347 - val_loss: 0.2137 - val_custom_rmse: 0.4897\n",
            "Epoch 196/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2040 - custom_rmse: 0.5389 - val_loss: 0.2132 - val_custom_rmse: 0.4881\n",
            "Epoch 197/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2041 - custom_rmse: 0.5349 - val_loss: 0.2134 - val_custom_rmse: 0.4879\n",
            "Epoch 198/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2041 - custom_rmse: 0.5342 - val_loss: 0.2136 - val_custom_rmse: 0.4886\n",
            "Epoch 199/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2045 - custom_rmse: 0.5339 - val_loss: 0.2132 - val_custom_rmse: 0.4883\n",
            "Epoch 200/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2044 - custom_rmse: 0.5317 - val_loss: 0.2134 - val_custom_rmse: 0.4883\n",
            "Epoch 201/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2040 - custom_rmse: 0.5266 - val_loss: 0.2131 - val_custom_rmse: 0.4879\n",
            "Epoch 202/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2036 - custom_rmse: 0.5372 - val_loss: 0.2129 - val_custom_rmse: 0.4883\n",
            "Epoch 203/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2037 - custom_rmse: 0.5331 - val_loss: 0.2137 - val_custom_rmse: 0.4883\n",
            "Epoch 204/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2037 - custom_rmse: 0.5321 - val_loss: 0.2134 - val_custom_rmse: 0.4872\n",
            "Epoch 205/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2036 - custom_rmse: 0.5328 - val_loss: 0.2124 - val_custom_rmse: 0.4865\n",
            "Epoch 206/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2032 - custom_rmse: 0.5325 - val_loss: 0.2128 - val_custom_rmse: 0.4870\n",
            "Epoch 207/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2035 - custom_rmse: 0.5303 - val_loss: 0.2131 - val_custom_rmse: 0.4872\n",
            "Epoch 208/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2034 - custom_rmse: 0.5341 - val_loss: 0.2132 - val_custom_rmse: 0.4868\n",
            "Epoch 209/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2032 - custom_rmse: 0.5362 - val_loss: 0.2120 - val_custom_rmse: 0.4863\n",
            "Epoch 210/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2033 - custom_rmse: 0.5234 - val_loss: 0.2142 - val_custom_rmse: 0.4868\n",
            "Epoch 211/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2039 - custom_rmse: 0.5302 - val_loss: 0.2141 - val_custom_rmse: 0.4867\n",
            "Epoch 212/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2038 - custom_rmse: 0.5272 - val_loss: 0.2128 - val_custom_rmse: 0.4862\n",
            "Epoch 213/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2040 - custom_rmse: 0.5330 - val_loss: 0.2127 - val_custom_rmse: 0.4861\n",
            "Epoch 214/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2036 - custom_rmse: 0.5299 - val_loss: 0.2136 - val_custom_rmse: 0.4863\n",
            "Epoch 215/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2037 - custom_rmse: 0.5304 - val_loss: 0.2129 - val_custom_rmse: 0.4864\n",
            "Epoch 216/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2027 - custom_rmse: 0.5320 - val_loss: 0.2120 - val_custom_rmse: 0.4857\n",
            "Epoch 217/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2026 - custom_rmse: 0.5291 - val_loss: 0.2127 - val_custom_rmse: 0.4864\n",
            "Epoch 218/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2025 - custom_rmse: 0.5294 - val_loss: 0.2121 - val_custom_rmse: 0.4853\n",
            "Epoch 219/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2025 - custom_rmse: 0.5321 - val_loss: 0.2120 - val_custom_rmse: 0.4847\n",
            "Epoch 220/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2029 - custom_rmse: 0.5295 - val_loss: 0.2132 - val_custom_rmse: 0.4851\n",
            "Epoch 221/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2034 - custom_rmse: 0.5188 - val_loss: 0.2126 - val_custom_rmse: 0.4849\n",
            "Epoch 222/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2025 - custom_rmse: 0.5324 - val_loss: 0.2123 - val_custom_rmse: 0.4855\n",
            "Epoch 223/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2027 - custom_rmse: 0.5309 - val_loss: 0.2118 - val_custom_rmse: 0.4852\n",
            "Epoch 224/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2020 - custom_rmse: 0.5308 - val_loss: 0.2125 - val_custom_rmse: 0.4858\n",
            "Epoch 225/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2024 - custom_rmse: 0.5288 - val_loss: 0.2120 - val_custom_rmse: 0.4856\n",
            "Epoch 226/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2022 - custom_rmse: 0.5273 - val_loss: 0.2120 - val_custom_rmse: 0.4854\n",
            "Epoch 227/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2018 - custom_rmse: 0.5317 - val_loss: 0.2120 - val_custom_rmse: 0.4852\n",
            "Epoch 228/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2015 - custom_rmse: 0.5285 - val_loss: 0.2119 - val_custom_rmse: 0.4845\n",
            "Epoch 229/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2014 - custom_rmse: 0.5323 - val_loss: 0.2123 - val_custom_rmse: 0.4845\n",
            "Epoch 230/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2019 - custom_rmse: 0.5304 - val_loss: 0.2133 - val_custom_rmse: 0.4847\n",
            "Epoch 231/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2025 - custom_rmse: 0.5259 - val_loss: 0.2119 - val_custom_rmse: 0.4843\n",
            "Epoch 232/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2017 - custom_rmse: 0.5262 - val_loss: 0.2117 - val_custom_rmse: 0.4835\n",
            "Epoch 233/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2018 - custom_rmse: 0.5252 - val_loss: 0.2117 - val_custom_rmse: 0.4835\n",
            "Epoch 234/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2016 - custom_rmse: 0.5219 - val_loss: 0.2115 - val_custom_rmse: 0.4838\n",
            "Epoch 235/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2010 - custom_rmse: 0.5255 - val_loss: 0.2116 - val_custom_rmse: 0.4835\n",
            "Epoch 236/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2013 - custom_rmse: 0.5287 - val_loss: 0.2110 - val_custom_rmse: 0.4836\n",
            "Epoch 237/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2009 - custom_rmse: 0.5304 - val_loss: 0.2114 - val_custom_rmse: 0.4837\n",
            "Epoch 238/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2009 - custom_rmse: 0.5297 - val_loss: 0.2108 - val_custom_rmse: 0.4830\n",
            "Epoch 239/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2006 - custom_rmse: 0.5258 - val_loss: 0.2112 - val_custom_rmse: 0.4832\n",
            "Epoch 240/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2006 - custom_rmse: 0.5309 - val_loss: 0.2105 - val_custom_rmse: 0.4829\n",
            "Epoch 241/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2002 - custom_rmse: 0.5253 - val_loss: 0.2112 - val_custom_rmse: 0.4831\n",
            "Epoch 242/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.2006 - custom_rmse: 0.5308 - val_loss: 0.2122 - val_custom_rmse: 0.4833\n",
            "Epoch 243/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2011 - custom_rmse: 0.5251 - val_loss: 0.2111 - val_custom_rmse: 0.4829\n",
            "Epoch 244/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2003 - custom_rmse: 0.5262 - val_loss: 0.2115 - val_custom_rmse: 0.4825\n",
            "Epoch 245/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2005 - custom_rmse: 0.5297 - val_loss: 0.2108 - val_custom_rmse: 0.4822\n",
            "Epoch 246/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2004 - custom_rmse: 0.5235 - val_loss: 0.2109 - val_custom_rmse: 0.4824\n",
            "Epoch 247/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2008 - custom_rmse: 0.5167 - val_loss: 0.2121 - val_custom_rmse: 0.4837\n",
            "Epoch 248/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2006 - custom_rmse: 0.5268 - val_loss: 0.2115 - val_custom_rmse: 0.4824\n",
            "Epoch 249/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2006 - custom_rmse: 0.5255 - val_loss: 0.2112 - val_custom_rmse: 0.4825\n",
            "Epoch 250/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1997 - custom_rmse: 0.5306 - val_loss: 0.2104 - val_custom_rmse: 0.4825\n",
            "Epoch 251/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.2000 - custom_rmse: 0.5318 - val_loss: 0.2106 - val_custom_rmse: 0.4820\n",
            "Epoch 252/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1999 - custom_rmse: 0.5285 - val_loss: 0.2105 - val_custom_rmse: 0.4825\n",
            "Epoch 253/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1994 - custom_rmse: 0.5236 - val_loss: 0.2107 - val_custom_rmse: 0.4822\n",
            "Epoch 254/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1994 - custom_rmse: 0.5265 - val_loss: 0.2107 - val_custom_rmse: 0.4823\n",
            "Epoch 255/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1994 - custom_rmse: 0.5293 - val_loss: 0.2106 - val_custom_rmse: 0.4812\n",
            "Epoch 256/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1997 - custom_rmse: 0.5233 - val_loss: 0.2106 - val_custom_rmse: 0.4824\n",
            "Epoch 257/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1995 - custom_rmse: 0.5248 - val_loss: 0.2106 - val_custom_rmse: 0.4820\n",
            "Epoch 258/1000\n",
            "3195/3195 [==============================] - 0s 16us/step - loss: 0.1994 - custom_rmse: 0.5268 - val_loss: 0.2104 - val_custom_rmse: 0.4817\n",
            "Epoch 259/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1993 - custom_rmse: 0.5272 - val_loss: 0.2103 - val_custom_rmse: 0.4818\n",
            "Epoch 260/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1995 - custom_rmse: 0.5272 - val_loss: 0.2103 - val_custom_rmse: 0.4819\n",
            "Epoch 261/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1994 - custom_rmse: 0.5269 - val_loss: 0.2105 - val_custom_rmse: 0.4816\n",
            "Epoch 262/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2001 - custom_rmse: 0.5260 - val_loss: 0.2109 - val_custom_rmse: 0.4817\n",
            "Epoch 263/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1996 - custom_rmse: 0.5257 - val_loss: 0.2111 - val_custom_rmse: 0.4819\n",
            "Epoch 264/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.2001 - custom_rmse: 0.5244 - val_loss: 0.2118 - val_custom_rmse: 0.4816\n",
            "Epoch 265/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.2004 - custom_rmse: 0.5273 - val_loss: 0.2121 - val_custom_rmse: 0.4830\n",
            "Epoch 266/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.2001 - custom_rmse: 0.5267 - val_loss: 0.2110 - val_custom_rmse: 0.4805\n",
            "Epoch 267/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1996 - custom_rmse: 0.5235 - val_loss: 0.2108 - val_custom_rmse: 0.4809\n",
            "Epoch 268/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1992 - custom_rmse: 0.5255 - val_loss: 0.2110 - val_custom_rmse: 0.4822\n",
            "Epoch 269/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1992 - custom_rmse: 0.5266 - val_loss: 0.2097 - val_custom_rmse: 0.4805\n",
            "Epoch 270/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1987 - custom_rmse: 0.5241 - val_loss: 0.2105 - val_custom_rmse: 0.4806\n",
            "Epoch 271/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1985 - custom_rmse: 0.5268 - val_loss: 0.2108 - val_custom_rmse: 0.4808\n",
            "Epoch 272/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1989 - custom_rmse: 0.5255 - val_loss: 0.2106 - val_custom_rmse: 0.4802\n",
            "Epoch 273/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1985 - custom_rmse: 0.5223 - val_loss: 0.2097 - val_custom_rmse: 0.4798\n",
            "Epoch 274/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1983 - custom_rmse: 0.5273 - val_loss: 0.2098 - val_custom_rmse: 0.4805\n",
            "Epoch 275/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1989 - custom_rmse: 0.5279 - val_loss: 0.2104 - val_custom_rmse: 0.4804\n",
            "Epoch 276/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1989 - custom_rmse: 0.5269 - val_loss: 0.2105 - val_custom_rmse: 0.4802\n",
            "Epoch 277/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1986 - custom_rmse: 0.5147 - val_loss: 0.2102 - val_custom_rmse: 0.4801\n",
            "Epoch 278/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1983 - custom_rmse: 0.5260 - val_loss: 0.2105 - val_custom_rmse: 0.4800\n",
            "Epoch 279/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1985 - custom_rmse: 0.5248 - val_loss: 0.2097 - val_custom_rmse: 0.4798\n",
            "Epoch 280/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1983 - custom_rmse: 0.5271 - val_loss: 0.2093 - val_custom_rmse: 0.4798\n",
            "Epoch 281/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1982 - custom_rmse: 0.5250 - val_loss: 0.2111 - val_custom_rmse: 0.4803\n",
            "Epoch 282/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1991 - custom_rmse: 0.5170 - val_loss: 0.2102 - val_custom_rmse: 0.4795\n",
            "Epoch 283/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1984 - custom_rmse: 0.5274 - val_loss: 0.2096 - val_custom_rmse: 0.4787\n",
            "Epoch 284/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1977 - custom_rmse: 0.5270 - val_loss: 0.2100 - val_custom_rmse: 0.4792\n",
            "Epoch 285/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1980 - custom_rmse: 0.5137 - val_loss: 0.2102 - val_custom_rmse: 0.4791\n",
            "Epoch 286/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1980 - custom_rmse: 0.5233 - val_loss: 0.2094 - val_custom_rmse: 0.4786\n",
            "Epoch 287/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1975 - custom_rmse: 0.5261 - val_loss: 0.2100 - val_custom_rmse: 0.4792\n",
            "Epoch 288/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1978 - custom_rmse: 0.5216 - val_loss: 0.2092 - val_custom_rmse: 0.4788\n",
            "Epoch 289/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1976 - custom_rmse: 0.5163 - val_loss: 0.2095 - val_custom_rmse: 0.4786\n",
            "Epoch 290/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1978 - custom_rmse: 0.5217 - val_loss: 0.2096 - val_custom_rmse: 0.4788\n",
            "Epoch 291/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1985 - custom_rmse: 0.5261 - val_loss: 0.2096 - val_custom_rmse: 0.4790\n",
            "Epoch 292/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1978 - custom_rmse: 0.5177 - val_loss: 0.2095 - val_custom_rmse: 0.4787\n",
            "Epoch 293/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1974 - custom_rmse: 0.5231 - val_loss: 0.2099 - val_custom_rmse: 0.4784\n",
            "Epoch 294/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1974 - custom_rmse: 0.5230 - val_loss: 0.2095 - val_custom_rmse: 0.4789\n",
            "Epoch 295/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1974 - custom_rmse: 0.5209 - val_loss: 0.2093 - val_custom_rmse: 0.4788\n",
            "Epoch 296/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1972 - custom_rmse: 0.5197 - val_loss: 0.2096 - val_custom_rmse: 0.4784\n",
            "Epoch 297/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1973 - custom_rmse: 0.5250 - val_loss: 0.2101 - val_custom_rmse: 0.4788\n",
            "Epoch 298/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1971 - custom_rmse: 0.5176 - val_loss: 0.2093 - val_custom_rmse: 0.4783\n",
            "Epoch 299/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1971 - custom_rmse: 0.5253 - val_loss: 0.2097 - val_custom_rmse: 0.4780\n",
            "Epoch 300/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1974 - custom_rmse: 0.5254 - val_loss: 0.2094 - val_custom_rmse: 0.4782\n",
            "Epoch 301/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1975 - custom_rmse: 0.5248 - val_loss: 0.2106 - val_custom_rmse: 0.4785\n",
            "Epoch 302/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1977 - custom_rmse: 0.5215 - val_loss: 0.2088 - val_custom_rmse: 0.4775\n",
            "Epoch 303/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1969 - custom_rmse: 0.5182 - val_loss: 0.2096 - val_custom_rmse: 0.4777\n",
            "Epoch 304/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1972 - custom_rmse: 0.5194 - val_loss: 0.2092 - val_custom_rmse: 0.4776\n",
            "Epoch 305/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1971 - custom_rmse: 0.5227 - val_loss: 0.2093 - val_custom_rmse: 0.4769\n",
            "Epoch 306/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1970 - custom_rmse: 0.5211 - val_loss: 0.2096 - val_custom_rmse: 0.4779\n",
            "Epoch 307/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1970 - custom_rmse: 0.5255 - val_loss: 0.2102 - val_custom_rmse: 0.4775\n",
            "Epoch 308/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1974 - custom_rmse: 0.5208 - val_loss: 0.2098 - val_custom_rmse: 0.4780\n",
            "Epoch 309/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1968 - custom_rmse: 0.5213 - val_loss: 0.2097 - val_custom_rmse: 0.4770\n",
            "Epoch 310/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1970 - custom_rmse: 0.5204 - val_loss: 0.2093 - val_custom_rmse: 0.4772\n",
            "Epoch 311/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1974 - custom_rmse: 0.5263 - val_loss: 0.2105 - val_custom_rmse: 0.4781\n",
            "Epoch 312/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1974 - custom_rmse: 0.5131 - val_loss: 0.2093 - val_custom_rmse: 0.4776\n",
            "Epoch 313/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1974 - custom_rmse: 0.5123 - val_loss: 0.2098 - val_custom_rmse: 0.4772\n",
            "Epoch 314/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1970 - custom_rmse: 0.5152 - val_loss: 0.2094 - val_custom_rmse: 0.4774\n",
            "Epoch 315/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1966 - custom_rmse: 0.5249 - val_loss: 0.2091 - val_custom_rmse: 0.4770\n",
            "Epoch 316/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1963 - custom_rmse: 0.5230 - val_loss: 0.2089 - val_custom_rmse: 0.4770\n",
            "Epoch 317/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1964 - custom_rmse: 0.5236 - val_loss: 0.2088 - val_custom_rmse: 0.4768\n",
            "Epoch 318/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1963 - custom_rmse: 0.5192 - val_loss: 0.2091 - val_custom_rmse: 0.4773\n",
            "Epoch 319/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1962 - custom_rmse: 0.5222 - val_loss: 0.2085 - val_custom_rmse: 0.4769\n",
            "Epoch 320/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1956 - custom_rmse: 0.5232 - val_loss: 0.2085 - val_custom_rmse: 0.4768\n",
            "Epoch 321/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1963 - custom_rmse: 0.5187 - val_loss: 0.2089 - val_custom_rmse: 0.4764\n",
            "Epoch 322/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1968 - custom_rmse: 0.5149 - val_loss: 0.2089 - val_custom_rmse: 0.4764\n",
            "Epoch 323/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1971 - custom_rmse: 0.5192 - val_loss: 0.2093 - val_custom_rmse: 0.4764\n",
            "Epoch 324/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1963 - custom_rmse: 0.5178 - val_loss: 0.2094 - val_custom_rmse: 0.4767\n",
            "Epoch 325/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1964 - custom_rmse: 0.5202 - val_loss: 0.2094 - val_custom_rmse: 0.4769\n",
            "Epoch 326/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1964 - custom_rmse: 0.5206 - val_loss: 0.2097 - val_custom_rmse: 0.4767\n",
            "Epoch 327/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1962 - custom_rmse: 0.5237 - val_loss: 0.2088 - val_custom_rmse: 0.4762\n",
            "Epoch 328/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1956 - custom_rmse: 0.5186 - val_loss: 0.2081 - val_custom_rmse: 0.4764\n",
            "Epoch 329/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1950 - custom_rmse: 0.5206 - val_loss: 0.2079 - val_custom_rmse: 0.4756\n",
            "Epoch 330/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1954 - custom_rmse: 0.5196 - val_loss: 0.2082 - val_custom_rmse: 0.4766\n",
            "Epoch 331/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1952 - custom_rmse: 0.5187 - val_loss: 0.2079 - val_custom_rmse: 0.4755\n",
            "Epoch 332/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1950 - custom_rmse: 0.5188 - val_loss: 0.2085 - val_custom_rmse: 0.4761\n",
            "Epoch 333/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1949 - custom_rmse: 0.5175 - val_loss: 0.2080 - val_custom_rmse: 0.4757\n",
            "Epoch 334/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1952 - custom_rmse: 0.5166 - val_loss: 0.2086 - val_custom_rmse: 0.4767\n",
            "Epoch 335/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1955 - custom_rmse: 0.5177 - val_loss: 0.2085 - val_custom_rmse: 0.4763\n",
            "Epoch 336/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1952 - custom_rmse: 0.5184 - val_loss: 0.2078 - val_custom_rmse: 0.4752\n",
            "Epoch 337/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1950 - custom_rmse: 0.5233 - val_loss: 0.2089 - val_custom_rmse: 0.4759\n",
            "Epoch 338/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1956 - custom_rmse: 0.5217 - val_loss: 0.2089 - val_custom_rmse: 0.4757\n",
            "Epoch 339/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1953 - custom_rmse: 0.5120 - val_loss: 0.2079 - val_custom_rmse: 0.4757\n",
            "Epoch 340/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1950 - custom_rmse: 0.5168 - val_loss: 0.2080 - val_custom_rmse: 0.4758\n",
            "Epoch 341/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1951 - custom_rmse: 0.5149 - val_loss: 0.2080 - val_custom_rmse: 0.4752\n",
            "Epoch 342/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1951 - custom_rmse: 0.5176 - val_loss: 0.2081 - val_custom_rmse: 0.4753\n",
            "Epoch 343/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1948 - custom_rmse: 0.5193 - val_loss: 0.2082 - val_custom_rmse: 0.4758\n",
            "Epoch 344/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1950 - custom_rmse: 0.5201 - val_loss: 0.2081 - val_custom_rmse: 0.4756\n",
            "Epoch 345/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1949 - custom_rmse: 0.5155 - val_loss: 0.2075 - val_custom_rmse: 0.4754\n",
            "Epoch 346/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1947 - custom_rmse: 0.5175 - val_loss: 0.2082 - val_custom_rmse: 0.4755\n",
            "Epoch 347/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1947 - custom_rmse: 0.5215 - val_loss: 0.2083 - val_custom_rmse: 0.4753\n",
            "Epoch 348/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1947 - custom_rmse: 0.5105 - val_loss: 0.2079 - val_custom_rmse: 0.4747\n",
            "Epoch 349/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1949 - custom_rmse: 0.5194 - val_loss: 0.2079 - val_custom_rmse: 0.4749\n",
            "Epoch 350/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1945 - custom_rmse: 0.5151 - val_loss: 0.2072 - val_custom_rmse: 0.4747\n",
            "Epoch 351/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1949 - custom_rmse: 0.5195 - val_loss: 0.2077 - val_custom_rmse: 0.4748\n",
            "Epoch 352/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1948 - custom_rmse: 0.5120 - val_loss: 0.2084 - val_custom_rmse: 0.4750\n",
            "Epoch 353/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1947 - custom_rmse: 0.5158 - val_loss: 0.2081 - val_custom_rmse: 0.4750\n",
            "Epoch 354/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1946 - custom_rmse: 0.5191 - val_loss: 0.2081 - val_custom_rmse: 0.4745\n",
            "Epoch 355/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1945 - custom_rmse: 0.5164 - val_loss: 0.2080 - val_custom_rmse: 0.4748\n",
            "Epoch 356/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1945 - custom_rmse: 0.5219 - val_loss: 0.2083 - val_custom_rmse: 0.4751\n",
            "Epoch 357/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1945 - custom_rmse: 0.5145 - val_loss: 0.2073 - val_custom_rmse: 0.4743\n",
            "Epoch 358/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1939 - custom_rmse: 0.5195 - val_loss: 0.2087 - val_custom_rmse: 0.4753\n",
            "Epoch 359/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1948 - custom_rmse: 0.5183 - val_loss: 0.2088 - val_custom_rmse: 0.4761\n",
            "Epoch 360/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1945 - custom_rmse: 0.5184 - val_loss: 0.2079 - val_custom_rmse: 0.4752\n",
            "Epoch 361/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1942 - custom_rmse: 0.5179 - val_loss: 0.2075 - val_custom_rmse: 0.4741\n",
            "Epoch 362/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1940 - custom_rmse: 0.5138 - val_loss: 0.2083 - val_custom_rmse: 0.4749\n",
            "Epoch 363/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1945 - custom_rmse: 0.5203 - val_loss: 0.2087 - val_custom_rmse: 0.4749\n",
            "Epoch 364/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1945 - custom_rmse: 0.5163 - val_loss: 0.2087 - val_custom_rmse: 0.4742\n",
            "Epoch 365/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1947 - custom_rmse: 0.5190 - val_loss: 0.2084 - val_custom_rmse: 0.4749\n",
            "Epoch 366/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1940 - custom_rmse: 0.5188 - val_loss: 0.2076 - val_custom_rmse: 0.4736\n",
            "Epoch 367/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1942 - custom_rmse: 0.5173 - val_loss: 0.2077 - val_custom_rmse: 0.4741\n",
            "Epoch 368/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1943 - custom_rmse: 0.5206 - val_loss: 0.2085 - val_custom_rmse: 0.4739\n",
            "Epoch 369/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1941 - custom_rmse: 0.5219 - val_loss: 0.2073 - val_custom_rmse: 0.4737\n",
            "Epoch 370/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1935 - custom_rmse: 0.5149 - val_loss: 0.2070 - val_custom_rmse: 0.4736\n",
            "Epoch 371/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1934 - custom_rmse: 0.5185 - val_loss: 0.2074 - val_custom_rmse: 0.4741\n",
            "Epoch 372/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1936 - custom_rmse: 0.5207 - val_loss: 0.2075 - val_custom_rmse: 0.4737\n",
            "Epoch 373/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1934 - custom_rmse: 0.5146 - val_loss: 0.2072 - val_custom_rmse: 0.4737\n",
            "Epoch 374/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1933 - custom_rmse: 0.5122 - val_loss: 0.2071 - val_custom_rmse: 0.4734\n",
            "Epoch 375/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1934 - custom_rmse: 0.5176 - val_loss: 0.2072 - val_custom_rmse: 0.4731\n",
            "Epoch 376/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1930 - custom_rmse: 0.5159 - val_loss: 0.2071 - val_custom_rmse: 0.4732\n",
            "Epoch 377/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1936 - custom_rmse: 0.5158 - val_loss: 0.2075 - val_custom_rmse: 0.4737\n",
            "Epoch 378/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1932 - custom_rmse: 0.5093 - val_loss: 0.2069 - val_custom_rmse: 0.4732\n",
            "Epoch 379/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1931 - custom_rmse: 0.5149 - val_loss: 0.2075 - val_custom_rmse: 0.4730\n",
            "Epoch 380/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1938 - custom_rmse: 0.5133 - val_loss: 0.2077 - val_custom_rmse: 0.4735\n",
            "Epoch 381/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1936 - custom_rmse: 0.5167 - val_loss: 0.2075 - val_custom_rmse: 0.4729\n",
            "Epoch 382/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1933 - custom_rmse: 0.5173 - val_loss: 0.2080 - val_custom_rmse: 0.4733\n",
            "Epoch 383/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1937 - custom_rmse: 0.5166 - val_loss: 0.2072 - val_custom_rmse: 0.4729\n",
            "Epoch 384/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1931 - custom_rmse: 0.5146 - val_loss: 0.2070 - val_custom_rmse: 0.4725\n",
            "Epoch 385/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1931 - custom_rmse: 0.5175 - val_loss: 0.2081 - val_custom_rmse: 0.4735\n",
            "Epoch 386/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1936 - custom_rmse: 0.5126 - val_loss: 0.2071 - val_custom_rmse: 0.4725\n",
            "Epoch 387/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1932 - custom_rmse: 0.5188 - val_loss: 0.2073 - val_custom_rmse: 0.4735\n",
            "Epoch 388/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1928 - custom_rmse: 0.5138 - val_loss: 0.2073 - val_custom_rmse: 0.4738\n",
            "Epoch 389/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1927 - custom_rmse: 0.5104 - val_loss: 0.2072 - val_custom_rmse: 0.4730\n",
            "Epoch 390/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1925 - custom_rmse: 0.5192 - val_loss: 0.2070 - val_custom_rmse: 0.4727\n",
            "Epoch 391/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1927 - custom_rmse: 0.5139 - val_loss: 0.2065 - val_custom_rmse: 0.4728\n",
            "Epoch 392/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1923 - custom_rmse: 0.5108 - val_loss: 0.2071 - val_custom_rmse: 0.4726\n",
            "Epoch 393/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1924 - custom_rmse: 0.5152 - val_loss: 0.2071 - val_custom_rmse: 0.4732\n",
            "Epoch 394/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1921 - custom_rmse: 0.5183 - val_loss: 0.2077 - val_custom_rmse: 0.4724\n",
            "Epoch 395/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1926 - custom_rmse: 0.5177 - val_loss: 0.2066 - val_custom_rmse: 0.4719\n",
            "Epoch 396/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1926 - custom_rmse: 0.5157 - val_loss: 0.2073 - val_custom_rmse: 0.4721\n",
            "Epoch 397/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1926 - custom_rmse: 0.5126 - val_loss: 0.2072 - val_custom_rmse: 0.4728\n",
            "Epoch 398/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1923 - custom_rmse: 0.5159 - val_loss: 0.2064 - val_custom_rmse: 0.4722\n",
            "Epoch 399/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1924 - custom_rmse: 0.5119 - val_loss: 0.2068 - val_custom_rmse: 0.4720\n",
            "Epoch 400/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1920 - custom_rmse: 0.5137 - val_loss: 0.2066 - val_custom_rmse: 0.4717\n",
            "Epoch 401/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1918 - custom_rmse: 0.5156 - val_loss: 0.2065 - val_custom_rmse: 0.4723\n",
            "Epoch 402/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1920 - custom_rmse: 0.5121 - val_loss: 0.2074 - val_custom_rmse: 0.4731\n",
            "Epoch 403/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1927 - custom_rmse: 0.5171 - val_loss: 0.2078 - val_custom_rmse: 0.4727\n",
            "Epoch 404/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1925 - custom_rmse: 0.5166 - val_loss: 0.2068 - val_custom_rmse: 0.4718\n",
            "Epoch 405/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1917 - custom_rmse: 0.5156 - val_loss: 0.2074 - val_custom_rmse: 0.4722\n",
            "Epoch 406/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1925 - custom_rmse: 0.5167 - val_loss: 0.2068 - val_custom_rmse: 0.4716\n",
            "Epoch 407/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1918 - custom_rmse: 0.5145 - val_loss: 0.2068 - val_custom_rmse: 0.4726\n",
            "Epoch 408/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1924 - custom_rmse: 0.5138 - val_loss: 0.2065 - val_custom_rmse: 0.4722\n",
            "Epoch 409/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1918 - custom_rmse: 0.5154 - val_loss: 0.2070 - val_custom_rmse: 0.4717\n",
            "Epoch 410/1000\n",
            "3195/3195 [==============================] - 0s 15us/step - loss: 0.1921 - custom_rmse: 0.5159 - val_loss: 0.2066 - val_custom_rmse: 0.4717\n",
            "Epoch 411/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1923 - custom_rmse: 0.5132 - val_loss: 0.2075 - val_custom_rmse: 0.4716\n",
            "Epoch 412/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1924 - custom_rmse: 0.5117 - val_loss: 0.2068 - val_custom_rmse: 0.4722\n",
            "Epoch 413/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1919 - custom_rmse: 0.5166 - val_loss: 0.2074 - val_custom_rmse: 0.4731\n",
            "Epoch 414/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1923 - custom_rmse: 0.5151 - val_loss: 0.2065 - val_custom_rmse: 0.4715\n",
            "Epoch 415/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1918 - custom_rmse: 0.5100 - val_loss: 0.2073 - val_custom_rmse: 0.4718\n",
            "Epoch 416/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1919 - custom_rmse: 0.5149 - val_loss: 0.2078 - val_custom_rmse: 0.4726\n",
            "Epoch 417/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1917 - custom_rmse: 0.5118 - val_loss: 0.2073 - val_custom_rmse: 0.4710\n",
            "Epoch 418/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1924 - custom_rmse: 0.5169 - val_loss: 0.2077 - val_custom_rmse: 0.4720\n",
            "Epoch 419/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1927 - custom_rmse: 0.5062 - val_loss: 0.2071 - val_custom_rmse: 0.4709\n",
            "Epoch 420/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1919 - custom_rmse: 0.5168 - val_loss: 0.2067 - val_custom_rmse: 0.4710\n",
            "Epoch 421/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1919 - custom_rmse: 0.5146 - val_loss: 0.2070 - val_custom_rmse: 0.4720\n",
            "Epoch 422/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1917 - custom_rmse: 0.5153 - val_loss: 0.2063 - val_custom_rmse: 0.4709\n",
            "Epoch 423/1000\n",
            "3195/3195 [==============================] - 0s 11us/step - loss: 0.1912 - custom_rmse: 0.5142 - val_loss: 0.2072 - val_custom_rmse: 0.4720\n",
            "Epoch 424/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1917 - custom_rmse: 0.5155 - val_loss: 0.2057 - val_custom_rmse: 0.4704\n",
            "Epoch 425/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1917 - custom_rmse: 0.5153 - val_loss: 0.2062 - val_custom_rmse: 0.4713\n",
            "Epoch 426/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1909 - custom_rmse: 0.5164 - val_loss: 0.2061 - val_custom_rmse: 0.4714\n",
            "Epoch 427/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1911 - custom_rmse: 0.5133 - val_loss: 0.2064 - val_custom_rmse: 0.4718\n",
            "Epoch 428/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1911 - custom_rmse: 0.5150 - val_loss: 0.2069 - val_custom_rmse: 0.4718\n",
            "Epoch 429/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1913 - custom_rmse: 0.5135 - val_loss: 0.2069 - val_custom_rmse: 0.4714\n",
            "Epoch 430/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1915 - custom_rmse: 0.5089 - val_loss: 0.2064 - val_custom_rmse: 0.4705\n",
            "Epoch 431/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1914 - custom_rmse: 0.5131 - val_loss: 0.2077 - val_custom_rmse: 0.4720\n",
            "Epoch 432/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1919 - custom_rmse: 0.5145 - val_loss: 0.2075 - val_custom_rmse: 0.4718\n",
            "Epoch 433/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1915 - custom_rmse: 0.5136 - val_loss: 0.2066 - val_custom_rmse: 0.4708\n",
            "Epoch 434/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1909 - custom_rmse: 0.5116 - val_loss: 0.2061 - val_custom_rmse: 0.4706\n",
            "Epoch 435/1000\n",
            "3195/3195 [==============================] - 0s 16us/step - loss: 0.1908 - custom_rmse: 0.5175 - val_loss: 0.2060 - val_custom_rmse: 0.4709\n",
            "Epoch 436/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1906 - custom_rmse: 0.5156 - val_loss: 0.2064 - val_custom_rmse: 0.4711\n",
            "Epoch 437/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1908 - custom_rmse: 0.5056 - val_loss: 0.2060 - val_custom_rmse: 0.4708\n",
            "Epoch 438/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1909 - custom_rmse: 0.5078 - val_loss: 0.2076 - val_custom_rmse: 0.4716\n",
            "Epoch 439/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1907 - custom_rmse: 0.5170 - val_loss: 0.2063 - val_custom_rmse: 0.4704\n",
            "Epoch 440/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1909 - custom_rmse: 0.5100 - val_loss: 0.2075 - val_custom_rmse: 0.4711\n",
            "Epoch 441/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1916 - custom_rmse: 0.5123 - val_loss: 0.2076 - val_custom_rmse: 0.4705\n",
            "Epoch 442/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1918 - custom_rmse: 0.5152 - val_loss: 0.2075 - val_custom_rmse: 0.4703\n",
            "Epoch 443/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1918 - custom_rmse: 0.5111 - val_loss: 0.2070 - val_custom_rmse: 0.4705\n",
            "Epoch 444/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1914 - custom_rmse: 0.5060 - val_loss: 0.2087 - val_custom_rmse: 0.4720\n",
            "Epoch 445/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1922 - custom_rmse: 0.5156 - val_loss: 0.2067 - val_custom_rmse: 0.4697\n",
            "Epoch 446/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1912 - custom_rmse: 0.5140 - val_loss: 0.2067 - val_custom_rmse: 0.4706\n",
            "Epoch 447/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1907 - custom_rmse: 0.5075 - val_loss: 0.2065 - val_custom_rmse: 0.4703\n",
            "Epoch 448/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1909 - custom_rmse: 0.5120 - val_loss: 0.2073 - val_custom_rmse: 0.4716\n",
            "Epoch 449/1000\n",
            "3195/3195 [==============================] - 0s 13us/step - loss: 0.1910 - custom_rmse: 0.5114 - val_loss: 0.2081 - val_custom_rmse: 0.4711\n",
            "Epoch 450/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1911 - custom_rmse: 0.5157 - val_loss: 0.2067 - val_custom_rmse: 0.4700\n",
            "Epoch 451/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1912 - custom_rmse: 0.5125 - val_loss: 0.2066 - val_custom_rmse: 0.4708\n",
            "Epoch 452/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1908 - custom_rmse: 0.5169 - val_loss: 0.2059 - val_custom_rmse: 0.4699\n",
            "Epoch 453/1000\n",
            "3195/3195 [==============================] - 0s 14us/step - loss: 0.1902 - custom_rmse: 0.5110 - val_loss: 0.2058 - val_custom_rmse: 0.4699\n",
            "Epoch 454/1000\n",
            "3195/3195 [==============================] - 0s 12us/step - loss: 0.1899 - custom_rmse: 0.5167 - val_loss: 0.2059 - val_custom_rmse: 0.4706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_LFkiNrVrEc",
        "colab_type": "text"
      },
      "source": [
        "# 訓練過程評估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3-B_Y1BTAAy",
        "colab_type": "code",
        "outputId": "c17e7825-2fd4-4871-ed1f-43fcce74d6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "n=1\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.plot(history.history['loss'][n:], color='b', label=\"Training loss\")\n",
        "ax.plot(history.history['val_loss'][n:], color='r', label=\"validation loss\")\n",
        "legend = ax.legend(loc='best', shadow=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxV1b338c/KPEIYAjKDiECYMUxF\nCqi1qFXrVFCsQ6287K211vo8xV5rrdV7tderVi+3re2jT3u1UqodqKI8tVIodYKgzGCCzGPCEAiB\nJCdZzx+/c5JDCBAh4WSffN+vV145Z5999l5nJee71l57nX2c9x4REQm+hFgXQEREmoYCXUQkTijQ\nRUTihAJdRCROKNBFROJEUqx23LFjR9+7d+9Y7V5EJJAKCgpKvPe5DT0Ws0Dv3bs3S5cujdXuRUQC\nyTm3+USPachFRCROKNBFROKEAl1EJE7EbAxdRM6+yspKNmzYQHl5eayLIqeQkZFB3759SUlJafRz\nFOgirciGDRvIycmhf//+JCToAL2lqqmpYffu3RQVFZGXl9fo5+kvKtKKlJeX07lzZ4V5C5eQkEDn\nzp0pLy9nxYoVjX9eM5ZJRFoghXkwJCQk4JzjnXfeobi4uHHPaeYyNbnFi+EHP4CqqliXRESk+Tnn\nOHToUKPWDVygv/cePPooVFTEuiQi8lnt3buX4cOHM3z4cM455xy6detWe7+ysrJR27j99ttZv379\nSdeZNWsWL7/8clMUmQsvvJCPP/64SbbV3AJ3UjQx0X5XV8e2HCLy2XXo0KE2HB9++GGysrK4//77\nj1nHe4/3/oRDQy+++OIp9/PNb37zzAsbQIHroSvQReJPZDbH9OnTGTRoEDt37mTGjBnk5+czaNAg\nHnnkkdp1Iz3mUChETk4OM2fOZNiwYYwbN449e/YA8OCDD/LMM8/Urj9z5kxGjx5N//79effddwE4\nfPgw1113HXl5eVx//fXk5+efsif+0ksvMWTIEAYPHsz3v/99AEKhEF/96ldrlz/77LMAPP300+Tl\n5TF06FBuvvnmJq+zhjSqh+6cmwL8FEgEfuW9f7yBdb4CPAx4YLn3/qYmLGctBbpI07j3XmjqkYTh\nwyGco5/ZunXr+M1vfkN+fj4Ajz/+OO3btycUCjF58mSuv/7646bwlZaWMnHiRB5//HHuu+8+Xnjh\nBWbOnHnctr33fPjhh8ydO5dHHnmEt956i+eee45zzjmH1157jeXLlzNy5MiTlm/btm08+OCDLF26\nlLZt23LJJZfw+uuvk5ubS0lJCStXrgTgwIEDAPzkJz9h8+bNpKSk1C5rbqfsoTvnEoFZwGVAHnCj\ncy6v3jr9gAeA8d77QcC9zVBWQIEuEq/69u1bG+YAr7zyCiNHjmTkyJGsXbuWNWvWHPec9PR0Lrvs\nMgAuuOACNm3a1OC2r7322uPWWbx4MdOmTQNg2LBhDBo06KTl++CDD7jooovo2LEjycnJ3HTTTSxa\ntIjzzjuP9evXc8899zB//nzatm0LwKBBg7j55pt5+eWXSU5O/kx1cboa00MfDRR57z8FcM7NBq4G\nomv3TmCW934/gPd+T1MXNEKBLtI0Trcn3VwyMzNrbxcWFvLTn/6UDz/8kJycHG6++WaOHj163HOi\nP0WZmJhIKBRqcNupqamnXOd0dejQgRUrVvDmm28ya9YsXnvtNZ5//nnmz5/PwoULmTt3Lv/2b//G\nihUrSIwEWDNpzBh6N2Br1P1t4WXRzgfOd8790zn3fniI5jjOuRnOuaXOuaWNnVdZnwJdJP4dPHiQ\n7Oxs2rRpw86dO5k/f36T72P8+PHMmTMHgJUrVzZ4BBBtzJgxLFiwgL179xIKhZg9ezYTJ06kuLgY\n7z033HADjzzyCMuWLaO6uppt27Zx0UUX8ZOf/ISSkpKzcrmFpprlkgT0AyYB3YFFzrkh3vtjBo68\n988DzwPk5+f709mRAl0k/o0cOZK8vDwGDBhAr169GD9+fJPv41vf+ha33HILeXl5tT+R4ZKGdO/e\nnR//+MdMmjQJ7z1XXnklV1xxBcuWLeOOO+7Ae49zjieeeIJQKMRNN93EoUOHqKmp4f777yc7O7vJ\nX0N9zvuT56pzbhzwsPf+i+H7DwB47/89ap2fAx94718M3/8bMNN7v+RE283Pz/en8wUXL70EX/0q\nfPIJ9Ov3mZ8u0qoVFBRwwQUXxLoYLUIoFCIUCpGWlkZhYSGXXnophYWFJCW1nNncBQUFLF68mCuv\nvJJzzz0XAOdcgfc+v6H1G1PyJUA/51wfYDswDag/g+VPwI3Ai865jtgQzKen+RpOSj10EWkKZWVl\nXHzxxYRCIbz3/OIXv2hRYX46Tll6733IOXc3MB+btviC9361c+4RYKn3fm74sUudc2uAauB/ee/3\nNkeBFegi0hRycnIoKCiIdTGaVKOaI+/9PGBevWUPRd32wH3hn2alQBcRaZg+KSoiEicU6CIicUKB\nLiISJxToItKiZWVlAbBjxw6uv/76BteZNGkSp5oG/cwzzxzz4Z7LL7+8Sa6x8vDDD/Pkk0+e8Xaa\nggJdRAKha9euvPrqq6f9/PqBPm/ePHJycpqiaC2GAl1EzpqZM2cya9as2vuR3m1kTvjIkSMZMmQI\nf/7zn4977qZNmxg8eDAAR44cYdq0aQwcOJBrrrmGI0eO1K73jW98o/ayuz/84Q8BePbZZ9mxYweT\nJ09m8uTJAPTu3ZuSkhIAnnrqKQYPHszgwYNrL7u7adMmBg4cyJ133smgQYO49NJLj9lPQz7++GPG\njh3L0KFDueaaa9i/f3/t/iOX0o1cEGzhwoW1X+4xYsSIRn8r0ckEbha9Al2kicTg+rlTp07l3nvv\nrf0Cijlz5jB//nzS0tL44x//SJs2bSgpKWHs2LFcddVVOOca3M7PfvYzMjIyWLt2LStWrDjm0reP\nPfYY7du3p7q6mosvvpgVK1Zwzz338NRTT7FgwQI6dux4zLYKCgp48cUX+eCDD/DeM2bMGCZOnEi7\ndu0oLCzklVde4Ze//CVf+cpXeO211056bfNbbrmF5557jokTJ/LQQw/xox/9iGeeeYbHH3+cjRs3\nkpqaWjvM8+STTzJr1izGjx9PWVkZaWlpja7mE1EPXUTOmhEjRrBnzx527NjB8uXLadeuHT169MB7\nz/e//32GDh3KJZdcwvbt29m9e/cJt7No0aLaYB06dChDhw6tfWzOnDmMHDmSESNGsHr16lNedGvx\n4sVcc801ZGZmkpWVxbXXXss//vEPAPr06cPw4cOBk1+eF+za7AcOHGDixIkA3HrrrSxatKi2jNOn\nT+ell16q/TTq+PHjue+++3j22Wc5cOBAk3xKVT10kdYqRtfPveGGG3j11VfZtWsXU6dOBeDll1+m\nuLiYgoICkpOT6d27d4OXyz2VjRs38uSTT7JkyRLatWvHbbfddlrbiYhcdhfs0runGnI5kTfeeINF\nixbxl7/8hccee4yVK1cyc+ZMrrjiCubNm8f48eOZP38+AwYMOO2ygnroInKWTZ06ldmzZ/Pqq69y\nww03ANa77dSpE8nJySxYsIDNmzefdBuf//zn+e1vfwvAqlWrWLFiBWCX3c3MzKRt27bs3r2bN998\ns/Y52dnZDY5TT5gwgT/96U+Ul5dz+PBh/vjHPzJhwoTP/Lratm1Lu3btanv3//M//8PEiROpqalh\n69atTJ48mSeeeILS0lLKysrYsGEDQ4YM4Xvf+x6jRo1i3bp1n3mf9amHLiJn1aBBgzh06BDdunWj\nS5cuAEyfPp0rr7ySIUOGkJ+ff8qe6je+8Q1uv/12Bg4cyMCBA2uvIDls2DBGjBjBgAED6NGjxzGX\n3Z0xYwZTpkyha9euLFiwoHb5yJEjue222xg9ejQAX//61xkxYsRJh1dO5Ne//jV33XUX5eXlnHvu\nubz44otUV1dz8803U1paiveee+65h5ycHH7wgx+wYMECEhISGDRoUO03L52JU14+t7mc7uVzly+3\n8y6vvQbhb5USkUbS5XOD5bNePldDLiIicUKBLiISJwIX6JGZPU38Pa8irUZNTU2siyCNcDp/p8AF\nunroIqcvIyODXbt2KdRbuJqaGnbt2kVVVdVnep5muYi0In379mXNmjXs2LHjhJ/ClJahqqqKLVu2\n4JwjIaFxfW8FukgrkpKSQrdu3XjllVdITU0lJSUl1kWSkzhy5AjJycl06NChUesr0EVamdzcXK6+\n+mref//90/7ko5wdnTt3ZsKECWRnZzdqfQW6SCvUq1cvevXqFetiSBPTSVERkTihQBcRiRMKdBGR\nOKFAFxGJEwp0EZE4EbhAT/rza7zB5VBREeuiiIi0KIEL9ITNG7mcN+EzfiRWRCTeBS7QXbJNna+p\n0piLiEi0wAV6ZBDdV+lyiyIi0YIb6CH10EVEogUv0HVBdBGRBgUv0NVDFxFpUPACPUknRUVEGhK8\nQI98skhDLiIixwhsoGvIRUTkWMELdJ0UFRFpUPACXT10EZEGBTbQdXUuEZFjNSrQnXNTnHPrnXNF\nzrmZDTx+m3Ou2Dn3cfjn601f1DANuYiINOiU3ynqnEsEZgFfALYBS5xzc733a+qt+jvv/d3NUMZj\nachFRKRBjemhjwaKvPefeu8rgdnA1c1brJMI99BdtXroIiLRGhPo3YCtUfe3hZfVd51zboVz7lXn\nXI+GNuScm+GcW+qcW1pcXHwaxaWuh64xdBGRYzTVSdG/AL2990OBvwK/bmgl7/3z3vt8731+bm7u\n6e2p9oNFCnQRkWiNCfTtQHSPu3t4WS3v/V7vfeQrhH4FXNA0xWuAhlxERBrUmEBfAvRzzvVxzqUA\n04C50Ss457pE3b0KWNt0RaxHJ0VFRBp0ylku3vuQc+5uYD6QCLzgvV/tnHsEWOq9nwvc45y7CggB\n+4Dbmq/E6qGLiDTklIEO4L2fB8yrt+yhqNsPAA80bdFOQD10EZEG6ZOiIiJxIniBrk+Kiog0KHiB\nrh66iEiDAhvoOikqInKs4AV6ZMhFPXQRkWMEL9A15CIi0qDgBbrmoYuINCh4ga4euohIgwIb6Oqh\ni4gcK3iBHhlyqVEPXUQkWvACXUMuIiINCl6gh3voCTUachERiRa8QI/00DXkIiJyjMAGeoJOioqI\nHCN4ge4cNS5BJ0VFROoJXqADNS4R56vxPtYlERFpOQIZ6D4hkSRC1NTEuiQiIi1HIAO9JiGJRKp1\nSXQRkSiBDPRID12BLiJSJ5iBnqgeuohIfcEM9IREBbqISD2BDXQNuYiIHCugga4hFxGR+oIZ6Inq\noYuI1BfQQFcPXUSkvmAGusbQRUSOE8hAJ1GzXERE6gtkoGvIRUTkeIEMdHRSVETkOMEM9KQkBbqI\nSD3BDPREBbqISH2BDHSfnEwyVQp0EZEogQx0FOgiIsdRoIuIxAkFuohInFCgi4jEiUAGulOgi4gc\np1GB7pyb4pxb75wrcs7NPMl61znnvHMuv+mK2IDUFFKoVKCLiEQ5ZaA75xKBWcBlQB5wo3Mur4H1\nsoFvAx80dSGP25d66CIix2lMD300UOS9/9R7XwnMBq5uYL0fA08AR5uwfA1yKQp0EZH6GhPo3YCt\nUfe3hZfVcs6NBHp4799owrKdkAJdROR4Z3xS1DmXADwFfLcR685wzi11zi0tLi4+7X0mpFqgV1ae\n9iZEROJOYwJ9O9Aj6n738LKIbGAw8Hfn3CZgLDC3oROj3vvnvff53vv83Nzc0y+0Al1E5DiNCfQl\nQD/nXB/nXAowDZgbedB7X+q97+i97+297w28D1zlvV/aLCUGEtMs0CsqmmsPIiLBc8pA996HgLuB\n+cBaYI73frVz7hHn3FXNXcCGJKYlk0gNlUdrYrF7EZEWKakxK3nv5wHz6i176ATrTjrzYp1cQmoy\nAKEjVUBqc+9ORCQQgvlJ0RQL9OqjVTEuiYhIyxHIQCc5uocuIiIQ8EBXD11EpE6gA73mqOYtiohE\nBDvQK9RDFxGJCGagp6QAGnIREYkWzEBXD11E5DgKdBGROBHoQPeVCnQRkYhAB7p66CIidQId6Oqh\ni4jUUaCLiMSJQAc6VQp0EZGIQAe6eugiInUCHegupEAXEYkIdKB7DbmIiNQKdKAnVOniXCIiEcEM\n9PC1XDTkIiJSJ5iBnp4OQHLoCN7HuCwiIi1EMAM9MxOADA4TCsW4LCIiLUQwAz09He8cmRymUsPo\nIiJAUAPdOaqSM8iijPLyWBdGRKRlCGagA9VpmWRymIMHY10SEZGWIbCBXpOuQBcRiRbYQPcZFuil\npbEuiYhIyxDYQCdTPXQRkWiBDfSE7CwFuohIlMAGemIbDbmIiEQLbKAntdWQi4hItMAGekJ2JlmU\nKdBFRMICG+guK5MspyEXEZGIwAY6mZlk+MMcLNXVuUREIOCBnkgNRw5UxLokIiItQnADPTsbgIpi\nDaKLiECQA719ewBCxftjXBARkZYh8IFes1eBLiICcRDoSYf26ZroIiLEQaC3Zx8lJTEui4hIC9Co\nQHfOTXHOrXfOFTnnZjbw+F3OuZXOuY+dc4udc3lNX9R6ogK9uLjZ9yYi0uKdMtCdc4nALOAyIA+4\nsYHA/q33foj3fjjwE+CpJi9pfTk5gAX6nj3NvjcRkRavMT300UCR9/5T730lMBu4OnoF73303MFM\noPk/7ZOYSHWbHAW6iEhYUiPW6QZsjbq/DRhTfyXn3DeB+4AU4KKGNuScmwHMAOjZs+dnLevx22vX\njnYH97Nt2xlvSkQk8JrspKj3fpb3vi/wPeDBE6zzvPc+33ufn5ube8b7TOjYni4peykqOuNNiYgE\nXmMCfTvQI+p+9/CyE5kNfPlMCtVo3brRJ2krhYVnZW8iIi1aYwJ9CdDPOdfHOZcCTAPmRq/gnOsX\ndfcK4OxE7Hnn0b1iA4Wf6AJdIiKnHEP33oecc3cD84FE4AXv/Wrn3CPAUu/9XOBu59wlQBWwH7i1\nOQtd67zzSK0+gt+5k7KyrmRlnZW9ioi0SI05KYr3fh4wr96yh6Juf7uJy9U4551nvyhixYqufO5z\nMSmFiEiLENxPikJtoJ/PJyxbFuOyiIjEWLADvVcvfFYW49KXU1AQ68KIiMRWsAM9IQE3YgSfSytg\n0SLwOjcqIq1YsAMd4IILOK/sYzZ/GuKTT2JdGBGR2Al+oI8eTXLVEYaxnHnzTr26iEi8Cn6gT5wI\nwNTOf+eNN2JcFhGRGAp+oHftCuefz1UZf2PRIjiorxgVkVYq+IEO8KUvcf7Wt8mq2sfrr8e6MCIi\nsREfgT59OgmhKu7M+T2zZ8e6MCIisREfgT5iBAwcyF1ZL/HGG+hyuiLSKsVHoDsH06fTZ9ti+voi\n/vM/Y10gEZGzLz4CHeD22yEpiVmD/ptZs9A10kWk1YmfQO/aFW64gYs/fZ7uybv5znf0yVERaV3i\nJ9ABHn6YhIqjvDbyMV5/HX7xi1gXSETk7ImvQD//fPja1xj+wc+5a9xyvvMdWLUq1oUSETk74ivQ\nAR59FJeby3M7r6NH9gGmToXy8lgXSkSk+cVfoHfqBHPmkLRtM+/2volP1oS4806oqYl1wUREmlf8\nBTrA+PEwaxYdl7zJu5+7n9/+Fi67DEpKYl0wEZHmE5+BDjBjBnznO4x696esmvRNlv69jClTYO/e\nWBdMRKR5xG+gA/zHf8DddzNo4c/Y3GkU6cvfZ9QoWLEi1gUTEWl68R3oiYnw3HPw9ttkVZfyj9A4\nbt37FOPGwe9+F+vCiYg0rfgO9IiLLoJPPoHrr+eHB7/LHzKmM3PaRq69FrZvj3XhRESaRusIdICs\nLJg9Gx58kEvL/kBRYn++OPdfuPX8d3noISgri3UBRUTOTOsJdLAhmB//GFdUROLXv8YM90veLh/P\nyB9/mX/pPpf7v+vZsyfWhRQROT2tK9AjunWDn/8ct3cvPPooX8p4h9+UXs2TTyWw8pwv8B+f/wtv\nv6256yISLM7H6ApW+fn5funSpTHZ93FCIZg1iwP/XIV/8y3alW3jQ0ZxJKMj7TqnkDJ5PH1vvZDk\nYXnQtm2sSysirZhzrsB7n9/gYwr0eg4fJvTwo5S8uQRXVEj7ih0kE6p9eEuvCWQO7EnixZPImHEz\nKQdLIDkZOneOYaFFpLVQoJ+ByoNHWfC7PWz9UwFVS5czcc8cOrObDuyrXSeUmkHV6AtJP7rfvj3p\n85+Ha6+F9PQYllxE4pECvQmVlMAbr3ty35tL8dvLOfrpdgazit5sohs7atcrz+lC6qRxJA4ZBCkp\n0K4d9OkDmzbBBRfAmDGxexEiElgK9GbivX1/aUkJvP46/P3/VeI/+pjkw/u5k18y1K2kry8ikQbO\nrqan24XErrkGJk60YRvvoXt3GDLEZuSIiNSjQD+LKiqgshLef99C/q0/HmHb1hpyOMDgpPWEOnXl\nC6mLGJK0hkHVK+m1413c0aPHbiQ7GwYNgssvt29i6tnTevgAgwdDWprd9t6+TzVy+9AhaNPm7L1Y\nETnrFOgxVF0N77wDBw/CwoWwbx/s2gVbt9qHV3PYz6js9YzpsYPx2SvI6tGODnvW0HvPh6Sv+/j4\nDWZlWc9+3z4L8wsvtIBfsQI2boR774XMTBvSOXwY5syBDh3sovDnnAOTJ0Nqqj1PRwEigaNAb6HW\nrLG83bULli+3TI7+Mo5OaQe5/cv7+WLHAnKyqzlSkUCXNW/Tq90hEtq1tVahsNB65tnZsG7diXd2\nzjl2qcmqKrvft68N82zebEcBo0ZBbq49vnWrDQlt3Wrj/hkZMHaszd9ftsyOGKZNs/MBu3bBhAl1\nRwoi0qwU6AFRXQ07d8LRo9ar//vfbdjm0KFj1zv/fDjvPMvww4fhkkugd284WOrp2v4o52fvpEf5\nejhyxIZtysuhfXs7TPjb3+zQYNEi69n37Anz59uytDT7NFXfvrbhtDQ7SZCYCAcOHFuIzExbB6xB\nyMuz+4mJts3sbPj0Uxg92vY/fz5MnWovsH17SEqC4mKb1z9xopV11izo398akv797SjjuuvOrLGo\nrrZGqH9/DUdJXFCgB1hVlY2kREZYiorgV7+C0lK7/kx1teVmff3728jKkSOWy23awLBhtnzoUMvR\nAQNs+wnUkJUYXjEUsidGq6y0w4kFCyxgly+HP/0JBg601mfVKtiwwTa6ZYsFdVUVdOxoRwBg26yo\nsHUiBa8vI+P47wvs2tVatJoaexEdOtic/y1brIHKybHtdehgP8uXQ48edtRQWGgNCdgw1YABtp0r\nrrCjke3bbZ3LL7cGpn17G9KqroannrIvSrnrLpulVH94qrDQXkunTvZa9+yxI5jG2L/fzqT369e4\n9UWiKNDjmPeWpaWllncbNlj2/vnPlkvp6fazd691VOuffwVrKKZOtV5/ebltJy3Nsq2qyrJu2DAb\ndq+utg50r16Q0JgLR+zbV1eI4mILzcOHrcAdO9o6b7xhy6ZNs3AuK4Onn7bw3bnTAjw52ZaXlMD6\n9TaEFBkiSk21x7Zvt3XXrbPb3btba5ifb43Sjh3WqEQf8mRnH38I1JB27eynWzfb54cfWvl69bKy\nFRXBuefClCkW9AcO2PLUVNtHmzb2mt9/346SKivtKqA5OdYwFhfbdNa+faGgwOorcu5jwQL41rds\nnW7drA579rS6OHDA/oC9e8Pq1VYniYnw0ktw443QpYuVd9EieywnB95808rZpUvdCfb6KiqsN5CT\n04g/spxNCnQBLPy3b4clS+z9WlhoubFyJbz1lnUy09PtfdyQzEwL8UOH7NxqXp6N+3fubJk5apQ1\nCrm5lmVJScfv/6wMtdfU2JFGSoodPfTvbwUCC8iVKy3Iysrs3MC771pvPzHRnltebr3zhQutQaiq\ngrVrLVB37IDdu+0IIC3NKvTIEQvjoiIbJwML/1DIKjq6FR040CovKcmCffVqW96mjZWhsZKSbPsR\nzlkF15eRYftv6MJEaWk2ayo11fbfpYv1CFavrvu+xmnTbGrtwYM2/HbkiL2mrVutgZo+3cpy8KD9\ng6SmWp0MGGCHgu++a0dpXbpYo713r/U4Lr/cGvtly6zRuOIK+Ogj+MIX7B+rvLxuZld5uf39Nm60\nv5lz9k+3YYP9HaKdqCEqLbX99elz7PJInTln54OOHrXbPXue+J+1utrqokePul7NkSP2/7Z/vz3v\n97+3xvfKK+3/Ze1amDSp4e19Rgp0aZSaGvv/LC+3902kd+49/PWv1nGsrLTA/vnPLdhHjrROdP1h\nn8REGy2pqLD39ZYtloWDB9s2vvxl64xWVlqWtGlj67z3nr1P8vMtUzMy6rL4rDUIZ6KmxgoZXdDK\nSqvQyJFKtMJCa0k/9zkL+LIyO6dQVmat5UcfwVe+Yq1wx44Whrt2WWVEprNWVtpRS9++VskpKfbH\nWbbMeu6dOtknmI8etaOXdu3sj7t9u90vL7fQLS+3EzQDBtjzSkvhv//72BbeOfunqK62I4ji4s9e\nR8nJdSfnu3a1EIzsI7L9qiorZ0qKNS4NDdGBNSLe2xFbmzbWqJaV2T9y7972ejIy7Kgkso+8PFt/\n+3ZrJLy39Ssq6rZ77rl2iFpSYv/k48dbiK9caX+XAwesEb/1Vvvnf/FFq6/6rr4a3n7bGrPJk+1Q\nuH1722bfvp+97miCQHfOTQF+CiQCv/LeP17v8fuArwMhoBj4mvd+88m2qUAPtuiODVjHq6DAzq1+\n9JG9/6qqLNg/+cRuDxlioV1RYR23xujSxd4369ZZp3DoUHs/9uxp2y4rs/dKRYVlVv/+tu/One09\nGpmss3Gj5V7fvpad2dm2/V27LJcijYbUs3+/hWRWVt1wjvcW5L16WQOTlWXDTDt2WGWPHWt/sH/+\n0/5gEyZYRR84YOc4brnFQq5tWxvHKy21xqxDBztUrKy0RmjbNvvHSUuzMB4zxgJ+1y77p0pNrfvD\nRWZcDRliZWjTxnoae/fa9oYOtZ5CcbEt27bNGoO0tLohsvz8uiG1d96xwI8cQYG9zrw8+2e76CJ4\n9VXbV0RCAsycaett2GD737jR9t2xo513inyjzs9+ZudnTsMZBbpzLhH4BPgCsA1YAtzovV8Ttc5k\n4APvfblz7hvAJO/91JNtV/3eJ9UAAAjTSURBVIHeuq1caUHcqZN1Jg8csPfB0KF29P7WW/beLyqy\n90a3bhbWy5dbp3b7dsuV9PS6oaDGXsu+Qwc7H1laakfCubmWA5062T7y8uy9uHChZcj559vnvDp1\nsvd+pBGrqDj+/LHEGe+t0erRw/5Bo08cRc7x5ORYL+JUs6iqq+1IynvrqbRvf1pFOtNAHwc87L3/\nYvj+AwDe+38/wfojgP/y3o8/2XYV6HImKivtyDz6/bVhg3XSEhLsdlqahW5KCgwfbkG8YgX84Q82\nrJmUZENAa9daQ1FdXTeaAfa8yspj95uRYUfrOTk2otGvn3USd++2EYJLLrHfF1xgndnIzMzsbLv9\n0Ue2/bVrrREZM8Z+unevG/Lav9+2EXlthw9bWdV4CJx5oF8PTPHefz18/6vAGO/93SdY/7+AXd77\nRxt4bAYwA6Bnz54XbN580lEZkbPu8GEbEk1JsRO8JSV2XnX9ejty2Lmz7rxoz54W0mlp1uEqLLRz\nos6deMgXrAEYMMBGDSLDtikp9rzMTDt316aNDRulp9fNkLz00rorNaek2PmG1FQbps3JscYmPd1e\nQ2GhbW/wYGsMIh8l6N3bGq3du+1IZNiwY4f7S0ttFGHQoACcr2ilThboSQ0tPIMd3QzkAxMbetx7\n/zzwPFgPvSn3LdIUMjOt5xzRubP9XHxx455fXW29+lWrrIedm2sNwaFDFrb9+llQZmTYesuX2wzI\nwkJbf9s2my20caM1FpFx/3377ENmztkQcKRsAHPnnv7rjcxkzMy0IaWFC62R6drVGpE+fewEdlKS\nNWCVlXb007mzDVtHJse0bWvrpKfb9ry3RuTIEVunXTt7zDlrNNLTrVEKhawBatNGDUhTaEygbwd6\nRN3vHl52DOfcJcC/AhO99xX1HxdpDRITLaxGjTr1uikptl5j1o2oqbHAdc565zU1Fv7l5XU/SUnW\n+96+3Y4okpLqjgQOHLDzER072sSZxYvtOfv22XmNO+6wXvzy5fac996DefNOqyoaFPl8WaSB2LSp\nbjq/c1auyOy+jh3rJqNkZtr5lJoa20b37nbuMiMDxo2r+0zXOefUvZ7I7KnIEdOWLTYUfqrptJEZ\nrJmZTfe6z5bGDLkkYSdFL8aCfAlwk/d+ddQ6I4BXsaGZwsbsWGPoIi1fVZWFe9u2FqyJiXbEsH27\n9eKhbjKJ9xbW+/dbgEamkh8+XDczceNGa8hyc20iTP/+dq6xoKBucsiSJbafinrdwsispcgVJxri\nnK0XGfKKlLeiou5Dy0lJNkzWtasdBR0+bOF97rm2/L33bD9Dh9oJ8aNH7YhkwAAbgtu717aVnW2T\nXQYMsKMO52yyzsqVdiSzcaPNahw71vazf7+dWxk9+syui9cU0xYvB57Bpi2+4L1/zDn3CLDUez/X\nOfc2MATYGX7KFu/9VSfbpgJdRE6mrMwai8hU87Q0ayCqq61BSU+3sC0osPuhkAXn0aP2+a29e+2I\nZN06e86ECRbWoVDdTMjI54dKS20b5eU2rbW42O5/8IHtc8+eus9xZWVZz7+q6vhp+ImJ1hAcPWpl\nr38JJLChraefhptuOr16OeMxdO/9PGBevWUPRd2+5PSKJiLSsKyshi93E/nMUESvXs1Xhupq29++\nfRbSHTvWzTaqqbEZjVu22BFMVZX16CNHLmCPl5RYDz8ry67AMHeuDRk1B31SVEQkQE7WQ2/M5ZVE\nRCQAFOgiInFCgS4iEicU6CIicUKBLiISJxToIiJxQoEuIhInFOgiInEiZh8scs4VA6d7/dyOQEkT\nFifoVB91VBd1VBfHipf66OW9z23ogZgF+plwzi090SelWiPVRx3VRR3VxbFaQ31oyEVEJE4o0EVE\n4kRQA/35WBeghVF91FFd1FFdHCvu6yOQY+giInK8oPbQRUSkHgW6iEicCFygO+emOOfWO+eKnHMz\nY12e5uace8E5t8c5typqWXvn3F+dc4Xh3+3Cy51z7tlw3axwzo2MXcmbnnOuh3NugXNujXNutXPu\n2+HlrbU+0pxzHzrnlofr40fh5X2ccx+EX/fvnHMp4eWp4ftF4cd7x7L8zcE5l+ic+8g593r4fquq\ni0AFunMuEZgFXAbkATc65/JiW6pm93+BKfWWzQT+5r3vB/wtfB+sXvqFf2YAPztLZTxbQsB3vfd5\nwFjgm+G/f2utjwrgIu/9MGA4MMU5NxZ4Anjae38esB+4I7z+HcD+8PKnw+vFm28Da6Put6668N4H\n5gcYB8yPuv8A8ECsy3UWXndvYFXU/fVAl/DtLsD68O1fADc2tF48/gB/Br6g+vAAGcAyYAz2acik\n8PLa9wwwHxgXvp0UXs/FuuxNWAfdsQb9IuB1wLW2ughUDx3oBmyNur8tvKy16ey93xm+vQvoHL7d\nauonfIg8AviAVlwf4SGGj4E9wF+BDcAB7334O+qPec219RF+vBTocHZL3KyeAf43UBO+34FWVhdB\nC3Spx1sXo1XNPXXOZQGvAfd67w9GP9ba6sN7X+29H471TkcDA2JcpJhwzn0J2OO9L4h1WWIpaIG+\nHegRdb97eFlrs9s51wUg/HtPeHnc149zLhkL85e9938IL2619RHhvT8ALMCGFXKcc0nhh6Jfc219\nhB9vC+w9y0VtLuOBq5xzm4DZ2LDLT2lldRG0QF8C9AufuU4BpgFzY1ymWJgL3Bq+fSs2lhxZfkt4\ndsdYoDRqKCLwnHMO+D/AWu/9U1EPtdb6yHXO5YRvp2PnE9ZiwX59eLX69RGpp+uBd8JHNIHnvX/A\ne9/de98by4V3vPfTaW11EetB/NM48XE58Ak2VvivsS7PWXi9rwA7gSpsDPAObKzvb0Ah8DbQPryu\nw2YBbQBWAvmxLn8T18WF2HDKCuDj8M/lrbg+hgIfhetjFfBQePm5wIdAEfB7IDW8PC18vyj8+Lmx\nfg3NVC+TgNdbY13oo/8iInEiaEMuIiJyAgp0EZE4oUAXEYkTCnQRkTihQBcRiRMKdBGROKFAFxGJ\nE/8fzhDMzmJ0+psAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQdp9-L3NoAY",
        "colab_type": "text"
      },
      "source": [
        "#預測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6BiDFQbu5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = test[y_cols].copy()\n",
        "y_pred[:] = sc_y.inverse_transform(model.predict(test[x_cols]))\n",
        "test[y_cols] = sc_y.inverse_transform(test[y_cols])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUn4PKu1iM-e",
        "colab_type": "text"
      },
      "source": [
        "# 評估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpJsBW_yiQjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric(y_test,y_pred,name):\n",
        "    res = pd.DataFrame()\n",
        "    res['R2'] = [r2_score(y_test, y_pred)]\n",
        "    res['RMSE'] = [sqrt(mean_squared_error(y_test,y_pred))]\n",
        "    res.index = [name]\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI-SV6jENoKY",
        "colab_type": "code",
        "outputId": "8eb0a1c6-d10a-45f3-fcd6-84c119876389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "res = pd.DataFrame()\n",
        "for y_name in y_cols:\n",
        "  res = res.append(metric(test[y_name],y_pred[y_name],y_name))\n",
        "res.loc['AVG'] = res.mean()\n",
        "res"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R2</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C5NP</th>\n",
              "      <td>0.861604</td>\n",
              "      <td>0.317919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C5IP</th>\n",
              "      <td>0.764240</td>\n",
              "      <td>0.248469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C5N</th>\n",
              "      <td>0.945130</td>\n",
              "      <td>0.032067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6NP</th>\n",
              "      <td>0.125958</td>\n",
              "      <td>0.378240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6IP</th>\n",
              "      <td>0.856731</td>\n",
              "      <td>0.395641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6N</th>\n",
              "      <td>0.853060</td>\n",
              "      <td>0.634804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6A</th>\n",
              "      <td>0.164247</td>\n",
              "      <td>0.156190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7NP</th>\n",
              "      <td>0.986725</td>\n",
              "      <td>0.253392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7IP</th>\n",
              "      <td>0.866494</td>\n",
              "      <td>0.403752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7N</th>\n",
              "      <td>0.959735</td>\n",
              "      <td>0.836224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7A</th>\n",
              "      <td>0.724195</td>\n",
              "      <td>0.244056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8NP</th>\n",
              "      <td>0.975595</td>\n",
              "      <td>0.269991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8IP</th>\n",
              "      <td>0.960653</td>\n",
              "      <td>0.309707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8N</th>\n",
              "      <td>0.916527</td>\n",
              "      <td>0.527233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8A</th>\n",
              "      <td>0.619376</td>\n",
              "      <td>0.501496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9NP</th>\n",
              "      <td>0.987478</td>\n",
              "      <td>0.178893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9IP</th>\n",
              "      <td>0.954959</td>\n",
              "      <td>0.335644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9N</th>\n",
              "      <td>0.691081</td>\n",
              "      <td>0.448831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9A</th>\n",
              "      <td>0.724760</td>\n",
              "      <td>0.317905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10NP</th>\n",
              "      <td>0.652644</td>\n",
              "      <td>0.124601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10IP</th>\n",
              "      <td>0.888455</td>\n",
              "      <td>0.341235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10N</th>\n",
              "      <td>0.736937</td>\n",
              "      <td>0.066970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10A</th>\n",
              "      <td>0.209139</td>\n",
              "      <td>0.207282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AVG</th>\n",
              "      <td>0.757640</td>\n",
              "      <td>0.327415</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             R2      RMSE\n",
              "C5NP   0.861604  0.317919\n",
              "C5IP   0.764240  0.248469\n",
              "C5N    0.945130  0.032067\n",
              "C6NP   0.125958  0.378240\n",
              "C6IP   0.856731  0.395641\n",
              "C6N    0.853060  0.634804\n",
              "C6A    0.164247  0.156190\n",
              "C7NP   0.986725  0.253392\n",
              "C7IP   0.866494  0.403752\n",
              "C7N    0.959735  0.836224\n",
              "C7A    0.724195  0.244056\n",
              "C8NP   0.975595  0.269991\n",
              "C8IP   0.960653  0.309707\n",
              "C8N    0.916527  0.527233\n",
              "C8A    0.619376  0.501496\n",
              "C9NP   0.987478  0.178893\n",
              "C9IP   0.954959  0.335644\n",
              "C9N    0.691081  0.448831\n",
              "C9A    0.724760  0.317905\n",
              "C10NP  0.652644  0.124601\n",
              "C10IP  0.888455  0.341235\n",
              "C10N   0.736937  0.066970\n",
              "C10A   0.209139  0.207282\n",
              "AVG    0.757640  0.327415"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0uIzO5AaRhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}